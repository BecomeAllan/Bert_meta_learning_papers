{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BecomeAllan/Bert_meta_learning_papers/blob/main/Meta_learning_EFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "trqLc9E8hce9"
      },
      "id": "trqLc9E8hce9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89316599-c2d8-4b73-a490-83a852024552",
      "metadata": {
        "id": "89316599-c2d8-4b73-a490-83a852024552"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.16.2\n",
        "!pip install torchmetrics==0.8.0\n",
        "\n",
        "!pip install matplotlib==3.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0699fcf5-202b-4163-a4f1-8af69ab36e41",
      "metadata": {
        "id": "0699fcf5-202b-4163-a4f1-8af69ab36e41"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.manifold import TSNE\n",
        "from copy import deepcopy, copy\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "from pprint import pprint\n",
        "import shutil\n",
        "import datetime\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "SEED = 2222\n",
        "\n",
        "gen_seed = torch.Generator().manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful Functions"
      ],
      "metadata": {
        "id": "q4OpUylcuk0u"
      },
      "id": "q4OpUylcuk0u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random"
      ],
      "metadata": {
        "id": "QZv8O2GzCar3"
      },
      "id": "QZv8O2GzCar3"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import json, pickle\n",
        "\n",
        "# Random seed function\n",
        "def random_seed(value):\n",
        "    torch.backends.cudnn.deterministic=True\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    np.random.seed(value)\n",
        "    random.seed(value)\n",
        "\n",
        "# Batch creation function\n",
        "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
        "    idxs = list(range(0,len(taskset)))\n",
        "    if is_shuffle:\n",
        "        random.shuffle(idxs)\n",
        "    for i in range(0,len(idxs), batch_size):\n",
        "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]\n"
      ],
      "metadata": {
        "id": "MWboPAJ0MR-P"
      },
      "execution_count": null,
      "outputs": [],
      "id": "MWboPAJ0MR-P"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9_MGXvEuQ35"
      },
      "source": [
        "## Diagnosis funs"
      ],
      "id": "q9_MGXvEuQ35"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyrU457ea0ws"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data, batch_size,tokenizer,max_seq_length,\n",
        "                 input = 'text', output = 'label',\n",
        "                 train_size_per_class = 5, global_datasets = False):\n",
        "  data = data.reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "  if global_datasets:\n",
        "    global data_train, data_test\n",
        "\n",
        "  data_train = data.groupby('label').sample(train_size_per_class, replace=False)\n",
        "  idex = data.index.isin(data_train.index)\n",
        "  data_test = data[~idex].reset_index()\n",
        "\n",
        "\n",
        "  # Train\n",
        "  ## Transforma em dataset\n",
        "  dataset_train = SLR_DataSet(\n",
        "    data = data_train.sample(frac=1),\n",
        "    input = input,\n",
        "    output = output,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length =max_seq_length)\n",
        "\n",
        "  # Test\n",
        "  # Dataloaders\n",
        "    ## Transforma em dataset\n",
        "  dataset_test = SLR_DataSet(\n",
        "    data = data_test,\n",
        "    input = input,\n",
        "    output = output,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length =max_seq_length)\n",
        "  \n",
        "  # Dataloaders\n",
        "  ## Treino \n",
        "  data_train_loader = DataLoader(dataset_train,\n",
        "                           shuffle=True,\n",
        "                           batch_size=batch_size['train']\n",
        "                                )\n",
        "  \n",
        "  if len(dataset_test) % batch_size['test'] == 1 :\n",
        "    data_test_loader = DataLoader(dataset_test,\n",
        "                                    batch_size=batch_size['test'],\n",
        "                                    drop_last=True)\n",
        "  else:\n",
        "    data_test_loader = DataLoader(dataset_test,\n",
        "                                    batch_size=batch_size['test'],\n",
        "                                    drop_last=False)\n",
        "\n",
        "  return data_train_loader, data_test_loader, data_train, data_test\n"
      ],
      "id": "EyrU457ea0ws"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53LKzmWYYum7"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def meta_train(data, model, device, Info, print_epoch =True, size_layer=0, Test_resource =None):\n",
        "\n",
        "  learner = Learner(model = model, device = device, **Info)\n",
        "  \n",
        "  # Testing tasks\n",
        "  if isinstance(Test_resource, pd.DataFrame):\n",
        "    test = MetaTask(Test_resource, num_task = 0, k_support=10, k_query=10,\n",
        "                  training=False, **Info)\n",
        "\n",
        "\n",
        "  torch.clear_autocast_cache()\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Meta epoca\n",
        "  for epoch in tqdm(range(Info['meta_epoch']), desc= \"Meta epoch \", ncols=80):\n",
        "    # print(\"Meta Epoca:\", epoch)\n",
        "      \n",
        "      # Tarefas de treino\n",
        "      train = MetaTask(data,\n",
        "                      num_task = Info['num_task_train'],\n",
        "                      k_support=Info['k_qry'],\n",
        "                      k_query=Info['k_spt'], **Info)\n",
        "\n",
        "      # Batchs de tarefas    \n",
        "      db = create_batch_of_tasks(train, is_shuffle = True, batch_size = Info[\"outer_batch_size\"])\n",
        "\n",
        "      if print_epoch:\n",
        "      # Outer loop bach training\n",
        "        for step, task_batch in enumerate(db):          \n",
        "            print(\"\\n-----------------Training Mode\",\"Meta_epoch:\", epoch ,\"-----------------\\n\")\n",
        "            # meta-feedfoward\n",
        "            acc = learner(task_batch, valid_train= print_epoch)\n",
        "            print('Step:', step, '\\ttraining Acc:', acc)\n",
        "        if isinstance(Test_resource, pd.DataFrame):\n",
        "          # Validating Model \n",
        "          if ((epoch+1) % 4) + step == 0:\n",
        "              random_seed(123)\n",
        "              print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
        "              db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
        "              acc_all_test = []\n",
        "\n",
        "              # Looping testing tasks\n",
        "              for test_batch in db_test:\n",
        "                  acc = learner(test_batch, training = False)\n",
        "                  acc_all_test.append(acc)\n",
        "\n",
        "              print('Test acc:', np.mean(acc_all_test))\n",
        "              del acc_all_test, db_test\n",
        "\n",
        "              # Restarting training randomly\n",
        "              random_seed(int(time.time() % 10))\n",
        "          \n",
        "        \n",
        "      else:\n",
        "        for step, task_batch in enumerate(db):\n",
        "            acc = learner(task_batch, print_epoch, valid_train= print_epoch)\n",
        "\n",
        "  torch.clear_autocast_cache()\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "id": "53LKzmWYYum7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4nKctl6csYB"
      },
      "outputs": [],
      "source": [
        "def train_loop(data_train_loader, data_test_loader, model, device, epoch = 4, lr = 1, print_info = True, name = 'name'):\n",
        "  # Inicia o modelo\n",
        "  model_meta = deepcopy(model)\n",
        "  optimizer = Adam(model_meta.parameters(), lr=lr)\n",
        "\n",
        "  model_meta.to(device)\n",
        "  model_meta.train()\n",
        "\n",
        "  # Loop de treino da tarefa\n",
        "  for i in range(0, epoch):\n",
        "      all_loss = []\n",
        "\n",
        "      # Inner training batch (support set)\n",
        "      for inner_step, batch in enumerate(data_train_loader):\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          input_ids, attention_mask,q_token_type_ids, label_id = batch\n",
        "          \n",
        "          # Feedfoward\n",
        "          loss, _, _ = model_meta(input_ids, attention_mask,q_token_type_ids, labels = label_id.squeeze())\n",
        "          \n",
        "          # Calcula gradientes\n",
        "          loss.backward()\n",
        "\n",
        "          # Atualiza os parametros\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          all_loss.append(loss.item())\n",
        "      \n",
        "\n",
        "      if (i % 2 == 0) & print_info:\n",
        "          print(\"Loss: \", np.mean(all_loss))\n",
        "\n",
        "\n",
        "  # Predicao no banco de teste\n",
        "  model_meta.eval()\n",
        "  all_loss = []\n",
        "  all_acc = []\n",
        "  features = []\n",
        "  labels = []\n",
        "  predi_logit = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for inner_step, batch in enumerate(tqdm(data_test_loader,\n",
        "                                              desc=\"Test validation | \" + name,\n",
        "                                              ncols=80)) :\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, attention_mask,q_token_type_ids, label_id = batch\n",
        "\n",
        "        # Predicoes\n",
        "        _, feature, prediction = model_meta(input_ids, attention_mask,q_token_type_ids, labels = label_id.squeeze())\n",
        "\n",
        "        prediction = prediction.detach().cpu().squeeze()\n",
        "        label_id = label_id.detach().cpu()\n",
        "        logit = feature[1].detach().cpu()\n",
        "        feature_lat = feature[0].detach().cpu()\n",
        "\n",
        "\n",
        "        labels.append(label_id.numpy().squeeze())\n",
        "        features.append(feature_lat.numpy())\n",
        "        predi_logit.append(logit.numpy())\n",
        "\n",
        "        acc = fn.accuracy(prediction, label_id).item()\n",
        "        all_acc.append(acc)\n",
        "      del input_ids, attention_mask, label_id, batch\n",
        "\n",
        "  if print_info:\n",
        "    print(\"acc:\", np.mean(all_acc))\n",
        "\n",
        "  model_meta.to('cpu')\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  del model_meta, optimizer\n",
        "\n",
        "\n",
        "  features = np.concatenate(np.array(features,dtype=object))\n",
        "  labels = np.concatenate(np.array(labels,dtype=object))\n",
        "  logits = np.concatenate(np.array(predi_logit,dtype=object))\n",
        "\n",
        "  features = torch.tensor(features.astype(np.float32)).detach().clone()\n",
        "  labels = torch.tensor(labels.astype(int)).detach().clone()\n",
        "  logits = torch.tensor(logits.astype(np.float32)).detach().clone()\n",
        "\n",
        "  # Reducao de dimensionalidade\n",
        "  X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                    init='random').fit_transform(features.detach().clone())\n",
        "\n",
        "  return logits.detach().clone(), X_embedded, labels.detach().clone(), features.detach().clone()"
      ],
      "id": "I4nKctl6csYB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbEOmxghkbG7"
      },
      "outputs": [],
      "source": [
        "def wss_calc(logit, labels, trsh = 0.5):\n",
        "  \n",
        "  # Predicao com base nos treshould\n",
        "  predict_trash = torch.sigmoid(logit).squeeze() >= trsh\n",
        "  CM = confusion_matrix(labels, predict_trash.to(int) )\n",
        "  tn, fp, fne, tp = CM.ravel()\n",
        "\n",
        "  P = (tp + fne)  \n",
        "  N = (tn + fp) \n",
        "  recall = tp/(tp+fne)\n",
        "\n",
        "  # Wss antigo\n",
        "  wss_old = (tn + fne)/len(labels) -(1- recall)\n",
        "\n",
        "  # WSS novo\n",
        "  wss_new = (tn/N - fne/P)\n",
        "\n",
        "  return {\n",
        "      \"wss\": round(wss_old,4),\n",
        "      \"awss\": round(wss_new,4),\n",
        "      \"R\": round(recall,4),\n",
        "      \"CM\": CM\n",
        "      }"
      ],
      "id": "QbEOmxghkbG7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKnM3-9Xjql8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from torchmetrics import functional as fn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def plot(logits, X_embedded, labels, tresh, show = True,\n",
        "         namefig = \"plot\", make_plot = True, print_stats = True, save = True):\n",
        "  col = pd.MultiIndex.from_tuples([\n",
        "                                   (\"Predict\", \"0\"),\n",
        "                                   (\"Predict\", \"1\")\n",
        "                                   ])\n",
        "  index = pd.MultiIndex.from_tuples([\n",
        "                                   (\"Real\", \"0\"),\n",
        "                                   (\"Real\", \"1\")\n",
        "                                   ])\n",
        "\n",
        "  predict = torch.sigmoid(logits).detach().clone()\n",
        "\n",
        "  roc_auc = dict()\n",
        "\n",
        "  fpr, tpr, thresholds = roc_curve(labels, predict.squeeze())\n",
        "\n",
        "  # Sem especificar o tresh\n",
        "  # WSS\n",
        "  ## indice do recall 0.95\n",
        "  idx_wss95 = sum(tpr < 0.95)\n",
        "  thresholds95 = thresholds[idx_wss95]\n",
        "\n",
        "  wss95_info = wss_calc(logits,labels, thresholds95 )\n",
        "  acc_wss95 = fn.accuracy(predict, labels, threshold=thresholds95)\n",
        "  f1_wss95 = fn.f1_score(predict, labels, threshold=thresholds95)\n",
        "\n",
        "\n",
        "  # Especificando o tresh\n",
        "  # Treshold avaliation\n",
        "\n",
        "\n",
        "  ## WSS\n",
        "  wss_info = wss_calc(logits,labels, tresh )\n",
        "  # Accuraci\n",
        "  acc_wssR = fn.accuracy(predict, labels, threshold=tresh)\n",
        "  f1_wssR = fn.f1_score(predict, labels, threshold=tresh)\n",
        "\n",
        "\n",
        "  metrics= {\n",
        "      # WSS\n",
        "      \"WSS@95\": wss95_info['wss'],\n",
        "      \"AWSS@95\": wss95_info['awss'],\n",
        "      \"WSS@R\": wss_info['wss'],\n",
        "      \"AWSS@R\": wss_info['awss'],\n",
        "      # Recall\n",
        "      \"Recall_WSS@95\": wss95_info['R'],\n",
        "      \"Recall_WSS@R\": wss_info['R'],\n",
        "      # acc\n",
        "      \"acc@95\": acc_wss95.item(),\n",
        "      \"acc@R\": acc_wssR.item(),\n",
        "      # f1\n",
        "      \"f1@95\": f1_wss95.item(),\n",
        "      \"f1@R\": f1_wssR.item(),\n",
        "      # treshould 95\n",
        "      \"treshould@95\": thresholds95\n",
        "  }\n",
        "\n",
        "  # print stats\n",
        "\n",
        "  if print_stats:\n",
        "    wss95= f\"WSS@95:{wss95_info['wss']}, R: {wss95_info['R']}\"\n",
        "    wss95_adj= f\"ASSWSS@95:{wss95_info['awss']}\"\n",
        "    print(wss95)\n",
        "    print(wss95_adj)\n",
        "    print('Acc.:', round(acc_wss95.item(), 4))\n",
        "    print('F1-score:', round(f1_wss95.item(), 4))\n",
        "    print(f\"Treshold to wss95: {round(thresholds95, 4)}\")\n",
        "    cm = pd.DataFrame(wss95_info['CM'],\n",
        "              index=index,\n",
        "              columns=col)\n",
        "    \n",
        "    print(\"\\nConfusion matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\n---Metrics with threshold:\", tresh, \"----\\n\")\n",
        "    wss= f\"WSS@R:{wss_info['wss']}, R: {wss_info['R']}\"\n",
        "    print(wss)\n",
        "    wss_adj= f\"AWSS@R:{wss_info['awss']}\"\n",
        "    print(wss_adj)\n",
        "    print('Acc.:', round(acc_wssR.item(), 4))\n",
        "    print('F1-score:', round(f1_wssR.item(), 4))\n",
        "    cm = pd.DataFrame(wss_info['CM'],\n",
        "                index=index,\n",
        "                columns=col)\n",
        "      \n",
        "    print(\"\\nConfusion matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "\n",
        "  # Graficos\n",
        "\n",
        "  if make_plot:\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(25,10))\n",
        "    alpha = torch.squeeze(predict).numpy()\n",
        "\n",
        "    # plots\n",
        "\n",
        "    p1 = sns.scatterplot(x=X_embedded[:, 0],\n",
        "                  y=X_embedded[:, 1],\n",
        "                  hue=labels,\n",
        "                  alpha=alpha, ax = axes[0]).set_title('Predictions-TSNE')\n",
        "    \n",
        "    t_wss = predict >= thresholds95\n",
        "    t_wss = t_wss.squeeze().numpy()\n",
        "\n",
        "    p2 = sns.scatterplot(x=X_embedded[t_wss, 0],\n",
        "                  y=X_embedded[t_wss, 1],\n",
        "                  hue=labels[t_wss],\n",
        "                  alpha=alpha[t_wss], ax = axes[1]).set_title('WSS@95')\n",
        "\n",
        "    t = predict >= tresh\n",
        "    t = t.squeeze().numpy()\n",
        "\n",
        "    p3 = sns.scatterplot(x=X_embedded[t, 0],\n",
        "                  y=X_embedded[t, 1],\n",
        "                  hue=labels[t],\n",
        "                  alpha=alpha[t], ax = axes[2]).set_title(f'Predictions-Treshold {tresh}')\n",
        "\n",
        "\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    lw = 2\n",
        "\n",
        "    axes[3].plot(\n",
        "      fpr,\n",
        "      tpr,\n",
        "      color=\"darkorange\",\n",
        "      lw=lw,\n",
        "      label=\"ROC curve (area = %0.2f)\" % roc_auc)\n",
        "    \n",
        "    axes[3].plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "    axes[3].axhline(y=0.95, color='r', linestyle='-')\n",
        "    axes[3].set(xlabel=\"False Positive Rate\", ylabel=\"True Positive Rate\", title= \"ROC\")\n",
        "    axes[3].legend(loc=\"lower right\")\n",
        "\n",
        "    if show:\n",
        "      plt.show()\n",
        "    \n",
        "    if save:\n",
        "      fig.savefig(namefig, dpi=fig.dpi)\n",
        "\n",
        "  return metrics\n",
        "\n",
        "def auc_plot(logits,labels, color = \"darkorange\", label = \"test\"):\n",
        "    predict = torch.sigmoid(logits).detach().clone()\n",
        "    fpr, tpr, thresholds = roc_curve(labels, predict.squeeze())\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    lw = 2\n",
        "\n",
        "    label = label + str(round(roc_auc,2))\n",
        "    # print(label)\n",
        "\n",
        "    plt.plot(\n",
        "      fpr,\n",
        "      tpr,\n",
        "      color=color,\n",
        "      lw=lw,\n",
        "      label= label \n",
        "      )\n",
        "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "    plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "\n"
      ],
      "id": "TKnM3-9Xjql8"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from torchmetrics import functional as fn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "\n",
        "class diagnosis():\n",
        "  def __init__(self, names, Valid_resource, batch_size_test, model,Info,start = 0):\n",
        "    self.names=names\n",
        "    self.Valid_resource=Valid_resource\n",
        "    self.batch_size_test=batch_size_test\n",
        "    self.model=model\n",
        "    self.start=start \n",
        "\n",
        "    self.value_trash = widgets.FloatText(\n",
        "        value=0.95,\n",
        "        description='tresh',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    self.valueb = widgets.IntText(\n",
        "        value=10,\n",
        "        description='size',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    self.train_b = widgets.Button(description=\"Train\")\n",
        "    self.next_b = widgets.Button(description=\"Next\")\n",
        "    self.eval_b = widgets.Button(description=\"Evaluation\")\n",
        "\n",
        "    self.hbox = widgets.HBox([self.train_b, self.valueb])\n",
        "\n",
        "    self.next_b.on_click(self.Next_button)\n",
        "    self.train_b.on_click(self.Train_button)\n",
        "    self.eval_b.on_click(self.Evaluation_button)\n",
        "\n",
        "\n",
        "  # Next button\n",
        "  def Next_button(self,p):\n",
        "    clear_output()\n",
        "    self.i=self.i+1\n",
        "\n",
        "    # global domain\n",
        "    self.domain = names[self.i]\n",
        "    print(\"Name:\", self.domain)\n",
        "\n",
        "    # global data\n",
        "    self.data = self.Valid_resource[self.Valid_resource['domain'] == self.domain]\n",
        "    print(self.data['label'].value_counts())\n",
        "\n",
        "    display(self.hbox)\n",
        "    display(self.next_b)\n",
        "\n",
        "  # Train button\n",
        "  def Train_button(self, y):\n",
        "    clear_output()\n",
        "    print(self.domain)\n",
        "\n",
        "    # Preparing data for training\n",
        "    self.data_train_loader, self.data_test_loader, self.data_train, self.data_test = prepare_data(self.data,\n",
        "              train_size_per_class = self.valueb.value,\n",
        "              batch_size = {'train': Info['inner_batch_size'],\n",
        "                            'test': batch_size_test},\n",
        "              max_seq_length = Info['max_seq_length'],\n",
        "              tokenizer = Info['tokenizer'],\n",
        "              input = \"text\",\n",
        "              output = \"label\")\n",
        "\n",
        "    self.logits, self.X_embedded, self.labels, self.features = train_loop(self.data_train_loader, self.data_test_loader,\n",
        "                                                        model, device,\n",
        "                                                        epoch = Info['inner_update_step'],\n",
        "                                                        lr=Info['inner_update_lr'],\n",
        "                                                        print_info=True,\n",
        "                                                        name = self.domain)\n",
        "\n",
        "    tresh_box = widgets.HBox([self.eval_b, self.value_trash])\n",
        "    display(self.hbox)\n",
        "    display(tresh_box)\n",
        "    display(self.next_b)\n",
        "\n",
        "  # Evaluation button\n",
        "  def Evaluation_button(self, te):\n",
        "    clear_output()\n",
        "    tresh_box = widgets.HBox([self.eval_b, self.value_trash])\n",
        "\n",
        "    print(self.domain)\n",
        "    # print(\"\\n\")\n",
        "    print(\"-------Train data-------\")\n",
        "    print(data_train['label'].value_counts())\n",
        "    print(\"-------Test data-------\")\n",
        "    print(data_test['label'].value_counts())\n",
        "    # print(\"\\n\")\n",
        "    \n",
        "    display(self.next_b)\n",
        "    display(tresh_box)\n",
        "    display(self.hbox)\n",
        "\n",
        "    \n",
        "    metrics = plot(self.logits, self.X_embedded, self.labels,\n",
        "                    tresh=Info['tresh'], show = True,\n",
        "                    # namefig= \"./\"+base_path +\"/\"+\"Results/size_layer/\"+ name_domain+'/' +str(n_layers) + '/img/' + str(attempt) + 'plots',\n",
        "                    namefig= 'test',\n",
        "                  make_plot = True,\n",
        "                  print_stats = True,\n",
        "                  save=False)\n",
        "\n",
        "  def __call__(self):\n",
        "    self.i= self.start-1\n",
        "\n",
        "    clear_output()\n",
        "    display(self.next_b)"
      ],
      "metadata": {
        "id": "8yBhi1nxheP-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8yBhi1nxheP-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulation attemps"
      ],
      "metadata": {
        "id": "wyT3Zmk_-s0s"
      },
      "id": "wyT3Zmk_-s0s"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def pipeline_simpulation(Valid_resource, names_to_valid, path_save, model, Info):\n",
        "  for name in names_to_valid:\n",
        "    name = re.sub(\"\\.csv\", \"\",name)\n",
        "    Path(path_save  + name + \"/img\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  roc_stats = defaultdict(lambda: defaultdict(\n",
        "      lambda: defaultdict(\n",
        "          list\n",
        "          )\n",
        "      )\n",
        "  )\n",
        "\n",
        "\n",
        "  n_attempt = 5\n",
        "\n",
        "  all_metrics = []\n",
        "  for name in names_to_valid:\n",
        "    # break\n",
        "    data = Valid_resource[Valid_resource['domain'] == name].reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "    for attempt in range(n_attempt):\n",
        "\n",
        "      data_train_loader, data_test_loader,  _ , _ = prepare_data(data,\n",
        "                train_size_per_class = Info['k_spt'],\n",
        "                batch_size = {'train': Info['inner_batch_size'],\n",
        "                              'test': 100},\n",
        "                max_seq_length = Info['max_seq_length'],\n",
        "                tokenizer = Info['tokenizer'],\n",
        "                input = \"text\",\n",
        "                output = \"label\")\n",
        "      \n",
        "\n",
        "\n",
        "      print(\"---\"*4,\"attempt\", attempt, \"---\"*4)\n",
        "      logits, X_embedded, labels, features = train_loop(data_train_loader, data_test_loader,\n",
        "                                                        model, device,\n",
        "                                                        epoch = Info['inner_update_step'],\n",
        "                                                        lr=Info['inner_update_lr'],\n",
        "                                                        print_info=False,\n",
        "                                                        name = name)\n",
        "      \n",
        "      \n",
        "      name_domain = re.sub(\"\\.csv\", \"\",name)\n",
        "\n",
        "      metrics = plot(logits, X_embedded, labels,\n",
        "                    tresh=Info['tresh'], show = False,\n",
        "                    namefig= path_save  + name_domain + \"/img/\" + str(attempt) + 'plots',\n",
        "        make_plot = True, print_stats = False, save =  True)\n",
        "\n",
        "      \n",
        "      fpr, tpr, _ = roc_curve(labels, torch.sigmoid(logits).squeeze())\n",
        "      \n",
        "      metrics['name'] = name_domain\n",
        "      metrics['layer_size'] = Info['bert_layers']\n",
        "      metrics['attempt'] = attempt\n",
        "      roc_stats[name_domain][str(Info['bert_layers'])]['fpr'].append(fpr.tolist())\n",
        "      roc_stats[name_domain][str(Info['bert_layers'])]['tpr'].append(tpr.tolist())\n",
        "      all_metrics.append(metrics)\n",
        "\n",
        "      pd.DataFrame(all_metrics).to_csv(path_save+ \"metrics.csv\")\n",
        "\n",
        "      roc_path =  path_save + \"roc_stats.json\"\n",
        "      with open(roc_path, 'w') as fp:\n",
        "          json.dump(roc_stats, fp)\n",
        "\n",
        "\n",
        "      del fpr, tpr, logits, X_embedded, labels\n",
        "      del features, metrics,  _\n",
        "\n",
        "\n",
        "  save_info = Info.copy()\n",
        "  save_info['model'] = initializer_model.tokenizer.name_or_path\n",
        "  save_info.pop(\"tokenizer\")\n",
        "  save_info.pop(\"bert_layers\")\n",
        "\n",
        "  info_path =  path_save+\"info.json\"\n",
        "  with open(info_path, 'w') as fp:\n",
        "      json.dump(save_info, fp)"
      ],
      "metadata": {
        "id": "EkVIovS4-wDD"
      },
      "id": "EkVIovS4-wDD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistics"
      ],
      "metadata": {
        "id": "tmpTVGPECdy2"
      },
      "id": "tmpTVGPECdy2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset statistics\n",
        "def load_data_statistics(paths):\n",
        "  size = []\n",
        "  pos = []\n",
        "  neg = []\n",
        "  for p in paths:\n",
        "    data = pd.read_csv(p) \n",
        "    data = data.dropna()\n",
        "    # Dataset size\n",
        "    size.append(len(data))\n",
        "    # Number of positive labels\n",
        "    pos.append(data['labels'].value_counts()[1])\n",
        "    # Number of negative labels\n",
        "    neg.append(data['labels'].value_counts()[0])\n",
        "  del data\n",
        "\n",
        "  info_load = pd.DataFrame({\n",
        "      \"size\":size,\n",
        "      \"pos\":pos,\n",
        "      \"neg\":neg,\n",
        "      \"names\":names,\n",
        "      \"paths\": paths })\n",
        "  return info_load\n",
        "\n",
        "# Loading the datasets\n",
        "def load_data(train_info_load):\n",
        "\n",
        "  col = ['abstract','title', 'labels', 'domain']\n",
        "\n",
        "  data_train = pd.DataFrame(columns=col)\n",
        "  for p in train_info_load['paths']:  \n",
        "    data_temp = pd.read_csv(p).loc[:, ['labels', 'title', 'abstract']]\n",
        "    data_temp = pd.read_csv(p).loc[:, ['labels', 'title', 'abstract']]\n",
        "    data_temp['domain'] = os.path.basename(p)\n",
        "    data_train = pd.concat([data_train, data_temp])\n",
        "    \n",
        "  data_train['text'] = data_train['title'] + data_train['abstract'].replace(np.nan, '')\n",
        "\n",
        "  return( data_train \\\n",
        "            .replace({\"labels\":{0:\"negative\", 1:'positive'}})\\\n",
        "            .rename({\"labels\":\"label\"} , axis=1)\\\n",
        "            .loc[ :,(\"text\",\"domain\",\"label\")]\n",
        "        )"
      ],
      "metadata": {
        "id": "Mcy0Dss_ChtU"
      },
      "id": "Mcy0Dss_ChtU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data\n",
        "\n",
        "The 64 topic-agnostic labeled datasets proposed can be downloaded and mounted below:"
      ],
      "metadata": {
        "id": "9kxoLXspGo2C"
      },
      "id": "9kxoLXspGo2C"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O data.zip https://www.dropbox.com/sh/or0eyfo8znyu2kp/AABxXJVII48U0vY8TT3Bbp6Ea?dl=0\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "1XJy380z06mX"
      },
      "id": "1XJy380z06mX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c246e204-6d19-4dbe-b089-d7834f5767ce",
      "metadata": {
        "id": "c246e204-6d19-4dbe-b089-d7834f5767ce"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "path = 'SLR_data'\n",
        "paths = glob.glob(f\"{path}/**/*.csv\", recursive=True)\n",
        "pprint(paths)\n",
        "\n",
        "names = [os.path.basename(p) for p in paths]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating text data"
      ],
      "metadata": {
        "id": "uZMuTuO6t1qq"
      },
      "id": "uZMuTuO6t1qq"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Untreated text data { display-mode: \"form\" }\n",
        "\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML, display, clear_output\n",
        "from pprint import pprint\n",
        "import unicodedata\n",
        "\n",
        "# Loading data\n",
        "def handle_data(path):\n",
        "  data = pd.read_csv(path)\n",
        "  print()\n",
        "  sample = data.sample(1)\n",
        "  text =  sample['abstract'].values[0]\n",
        "  label = sample['labels'].values[0]\n",
        "  new_text = unicodedata.normalize(\"NFKD\",str(text))\n",
        "  print(f\"Label = {label}\")\n",
        "  print(f\"len(text) = {len(new_text)}\")\n",
        "  pprint(new_text)\n",
        "  print()\n",
        "i=0\n",
        "\n",
        "# Next button\n",
        "def next_button(p):\n",
        "  global i\n",
        "  i=i+1\n",
        "  try:\n",
        "    clear_output()\n",
        "    display(hbox)\n",
        "    print(f\"File: {names[i]}\")\n",
        "    handle_data(paths[i])\n",
        "    global ref\n",
        "    ref = paths[i]\n",
        "    print(f\"path: {paths[i]}\")\n",
        "    # print(f\"Control number: {i}\")\n",
        "  except  Exception as inst:\n",
        "    print(inst)\n",
        "    i=len(paths)\n",
        "    print('End')\n",
        "\n",
        "# Previous Button\n",
        "def prev_button(p):\n",
        "  global i\n",
        "  i=i-1\n",
        "  try:\n",
        "    clear_output()\n",
        "    display(hbox)\n",
        "    print(f\"File: {names[i]}\")\n",
        "    handle_data(paths[i])\n",
        "    global ref\n",
        "    ref = paths[i]\n",
        "    print(f\"path: {paths[i]}\")\n",
        "    # print(f\"Control number: {i}\")\n",
        "  except  Exception as inst:\n",
        "    print(inst)\n",
        "    i=0\n",
        "    print('End')\n",
        "\n",
        "next_b = widgets.Button(description=\"Next\")\n",
        "previous_b = widgets.Button(description=\"Previous\")\n",
        "\n",
        "hbox = widgets.HBox([previous_b, next_b])\n",
        "display(hbox)\n",
        "\n",
        "next_b.on_click(next_button)\n",
        "previous_b.on_click(prev_button)"
      ],
      "metadata": {
        "id": "Qzsfno8qtzzz"
      },
      "id": "Qzsfno8qtzzz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treated text data\n"
      ],
      "metadata": {
        "id": "op1ERM4lqODR"
      },
      "id": "op1ERM4lqODR"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# Regex multiple replace function\n",
        "def multiple_replace(dict, text):\n",
        "\n",
        "  # Building regex from dict keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # Substitution\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) \n",
        "\n",
        "# Undesirable patterns within texts\n",
        "patterns = {\n",
        "    'CONCLUSIONS AND IMPLICATIONS':'',\n",
        "    'BACKGROUND AND PURPOSE':'',\n",
        "    'EXPERIMENTAL APPROACH':'',\n",
        "    'KEY RESULTS AEA':'',\n",
        "    '©':'',\n",
        "    '®':'',\n",
        "    'μ':'',\n",
        "    '(C)':'',\n",
        "    'OBJECTIVE:':'',\n",
        "    'MATERIALS AND METHODS:':'',\n",
        "    'SIGNIFICANCE:':'',\n",
        "    'BACKGROUND:':'',\n",
        "    'RESULTS:':'',\n",
        "    'METHODS:':'',\n",
        "    'CONCLUSIONS:':'',\n",
        "    'AIM:':'',\n",
        "    'STUDY DESIGN:':'',\n",
        "    'CLINICAL RELEVANCE:':'',\n",
        "    'CONCLUSION:':'',\n",
        "    'HYPOTHESIS:':'',\n",
        "    'CLINICAL RELEVANCE:':'',\n",
        "    'Questions/Purposes:':'',\n",
        "    'Introduction:':'',\n",
        "    'PURPOSE:':'',\n",
        "    'PATIENTS AND METHODS:':'',\n",
        "    'FINDINGS:':'',\n",
        "    'INTERPRETATIONS:':'',\n",
        "    'FUNDING:':'',\n",
        "    'PROGRESS:':'',\n",
        "    'CONTEXT:':'',\n",
        "    'MEASURES:':'',\n",
        "    'DESIGN:':'',\n",
        "    'BACKGROUND AND OBJECTIVES:':'',\n",
        "    '<p>':'',\n",
        "    '</p>':'',\n",
        "    '<<ETX>>':'',\n",
        "    '+/-':'',\n",
        "    }\n",
        " \n",
        "patterns = {x.lower():y for x,y in patterns.items()}"
      ],
      "metadata": {
        "id": "SQc7P9dOqQA0"
      },
      "id": "SQc7P9dOqQA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Treated text data { display-mode: \"form\" }\n",
        "\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import HTML, display, clear_output\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# undesirable patterns within texts\n",
        "patterns = {\n",
        "    'CONCLUSIONS AND IMPLICATIONS':'',\n",
        "    'BACKGROUND AND PURPOSE':'',\n",
        "    'EXPERIMENTAL APPROACH':'',\n",
        "    'KEY RESULTS AEA':'',\n",
        "    '©':'',\n",
        "    '®':'',\n",
        "    'μ':'',\n",
        "    '(C)':'',\n",
        "    'OBJECTIVE:':'',\n",
        "    'MATERIALS AND METHODS:':'',\n",
        "    'SIGNIFICANCE:':'',\n",
        "    'BACKGROUND:':'',\n",
        "    'RESULTS:':'',\n",
        "    'METHODS:':'',\n",
        "    'CONCLUSIONS:':'',\n",
        "    'AIM:':'',\n",
        "    'STUDY DESIGN:':'',\n",
        "    'CLINICAL RELEVANCE:':'',\n",
        "    'CONCLUSION:':'',\n",
        "    'HYPOTHESIS:':'',\n",
        "    'CLINICAL RELEVANCE:':'',\n",
        "    'Questions/Purposes:':'',\n",
        "    'Introduction:':'',\n",
        "    'PURPOSE:':'',\n",
        "    'PATIENTS AND METHODS:':'',\n",
        "    'FINDINGS:':'',\n",
        "    'INTERPRETATIONS:':'',\n",
        "    'FUNDING:':'',\n",
        "    'PROGRESS:':'',\n",
        "    'CONTEXT:':'',\n",
        "    'MEASURES:':'',\n",
        "    'DESIGN:':'',\n",
        "    'BACKGROUND AND OBJECTIVES:':'',\n",
        "    '<p>':'',\n",
        "    '</p>':'',\n",
        "    '<<ETX>>':'',\n",
        "    '+/-':'',\n",
        "    }\n",
        " \n",
        "patterns = {x.lower():y for x,y in patterns.items()}\n",
        "\n",
        "def multiple_replace(dict, text):\n",
        "  # Create a regular expression  from the dictionary keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # For each match, look-up corresponding value in dictionary\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) \n",
        "\n",
        "# Text processing\n",
        "def treat_text(text):\n",
        "  text = unicodedata.normalize(\"NFKD\",str(text))\n",
        "  text = multiple_replace(patterns,text.lower())\n",
        "  text = re.sub('(\\(.+\\))|(\\[.+\\])|( \\d )|(<)|(>)|(- )','', text)\n",
        "  text = re.sub('( +)',' ', text)\n",
        "  text = re.sub('(, ,)|(,,)',',', text)\n",
        "  text = re.sub('(%)|(per cent)',' percent', text)\n",
        "  return text\n",
        "\n",
        "def handle_data(path):\n",
        "  data = pd.read_csv(path)\n",
        "  # print('Columns:')\n",
        "  # print(data.columns)\n",
        "  print()\n",
        "  sample = data.sample(1)\n",
        "  text =  sample['abstract'].values[0]\n",
        "  label = sample['labels'].values[0]\n",
        "  new_text = treat_text(text)\n",
        "  print(f\"Label = {label}\")\n",
        "  print(f\"len(text) = {len(new_text)}\")\n",
        "  pprint(new_text)\n",
        "  print()\n",
        "\n",
        "i=0\n",
        "\n",
        "# Next button\n",
        "def next_button(p):\n",
        "  global i\n",
        "  i=i+1\n",
        "  try:\n",
        "    clear_output()\n",
        "    display(hbox)\n",
        "    print(f\"File: {names[i]}\")\n",
        "    handle_data(paths[i])\n",
        "    global ref\n",
        "    ref = paths[i]\n",
        "    print(f\"path: {paths[i]}\")\n",
        "    # print(f\"Control number: {i}\")\n",
        "  except  Exception as inst:\n",
        "    print(inst)\n",
        "    i=len(paths)\n",
        "    print('End')\n",
        "\n",
        "# Previous Button\n",
        "def prev_button(p):\n",
        "  global i\n",
        "  i=i-1\n",
        "  try:\n",
        "    clear_output()\n",
        "    display(hbox)\n",
        "    print(f\"File: {names[i]}\")\n",
        "    handle_data(paths[i])\n",
        "    global ref\n",
        "    ref = paths[i]\n",
        "    print(f\"path: {paths[i]}\")\n",
        "    # print(f\"Control number: {i}\")\n",
        "  except  Exception as inst:\n",
        "    print(inst)\n",
        "    i=0\n",
        "    print('End')\n",
        "\n",
        "next_b = widgets.Button(description=\"Next\")\n",
        "previous_b = widgets.Button(description=\"Previous\")\n",
        "\n",
        "hbox = widgets.HBox([previous_b, next_b])\n",
        "display(hbox)\n",
        "\n",
        "next_b.on_click(next_button)\n",
        "previous_b.on_click(prev_button)"
      ],
      "metadata": {
        "id": "v277Bsj1u9ZH"
      },
      "id": "v277Bsj1u9ZH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading pre-trained model\n"
      ],
      "metadata": {
        "id": "yV9633efG0Ql"
      },
      "id": "yV9633efG0Ql"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0430dcb-8618-4c65-8d4c-aa9634806c66",
      "metadata": {
        "id": "f0430dcb-8618-4c65-8d4c-aa9634806c66"
      },
      "outputs": [],
      "source": [
        "# Fetching pre-trained model and tokenizer\n",
        "\n",
        "class initializer:\n",
        "  def __init__(self, MODEL_NAME, **config):    \n",
        "    self.MODEL_NAME = MODEL_NAME\n",
        "\n",
        "    model = config.get(\"model\")\n",
        "    tokenizer = config.get(\"tokenizer\")\n",
        "\n",
        "    # Model\n",
        "    self.model = model.from_pretrained(MODEL_NAME, \n",
        "                                       return_dict=True,\n",
        "                                       output_attentions = False)\n",
        "    # Tokenizer\n",
        "    self.tokenizer = tokenizer.from_pretrained(MODEL_NAME,\n",
        "                                               do_lower_case = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0008c783-fd77-467a-a216-a4e650186603",
      "metadata": {
        "tags": [],
        "id": "0008c783-fd77-467a-a216-a4e650186603"
      },
      "outputs": [],
      "source": [
        "# Model and tokenizer of choice\n",
        "config = {\n",
        "    \"model\": AutoModelForSequenceClassification,\n",
        "    \"tokenizer\": AutoTokenizer\n",
        "     }\n",
        "\n",
        "# Pre-trained model initializer (uncased sciBERT)\n",
        "initializer_model = initializer('allenai/scibert_scivocab_uncased', **config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd636d02-621e-4c60-845a-ddc41aef74b5",
      "metadata": {
        "tags": [],
        "id": "dd636d02-621e-4c60-845a-ddc41aef74b5"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions based on the repository: https://github.com/mailong25/meta-learning-bert"
      ],
      "metadata": {
        "id": "VsSjC6d3MsDd"
      },
      "id": "VsSjC6d3MsDd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain Learner"
      ],
      "metadata": {
        "id": "cQoegkRPGA-b"
      },
      "id": "cQoegkRPGA-b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fdac80-9f2e-44d0-86ec-d93babf9d419",
      "metadata": {
        "id": "51fdac80-9f2e-44d0-86ec-d93babf9d419"
      },
      "outputs": [],
      "source": [
        "# Pre-trained model\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layers, freeze_bert, model):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    # Dummy Parameter\n",
        "    self.dummy_param = nn.Parameter(torch.empty(0))\n",
        "    \n",
        "    # Pre-trained model\n",
        "    self.model = deepcopy(model)\n",
        "\n",
        "    # Freezing bert parameters\n",
        "    if freeze_bert:\n",
        "      for param in self.model.parameters():\n",
        "        param.requires_grad = freeze_bert\n",
        "\n",
        "    # Selecting hidden layers of the pre-trained model\n",
        "    old_model_encoder = self.model.encoder.layer\n",
        "    new_model_encoder = nn.ModuleList()\n",
        "    \n",
        "    for i in layers:\n",
        "      new_model_encoder.append(old_model_encoder[i])\n",
        "\n",
        "    self.model.encoder.layer = new_model_encoder\n",
        "  \n",
        "  # Feed forward\n",
        "  def forward(self, **x):\n",
        "    return self.model(**x)['pooler_output']\n",
        "\n",
        "# Complete model\n",
        "class SLR_Classifier(nn.Module):\n",
        "  def __init__(self, **data):\n",
        "    super(SLR_Classifier, self).__init__()\n",
        "\n",
        "    # Dummy Parameter\n",
        "    self.dummy_param = nn.Parameter(torch.empty(0))\n",
        "\n",
        "    # Loss function\n",
        "    # Binary Cross Entropy with logits reduced to mean\n",
        "    self.loss_fn = nn.BCEWithLogitsLoss(reduction = 'mean',\n",
        "                                        pos_weight=torch.FloatTensor([data.get(\"pos_weight\",  2.5)]))\n",
        "\n",
        "    # Pre-trained model\n",
        "    self.Encoder = Encoder(layers = data.get(\"bert_layers\",  range(12)),\n",
        "                           freeze_bert = data.get(\"freeze_bert\",  False),\n",
        "                           model = data.get(\"model\"),\n",
        "                           )\n",
        "\n",
        "    # Feature Map Layer\n",
        "    self.feature_map = nn.Sequential(\n",
        "            # nn.LayerNorm(self.Encoder.model.config.hidden_size),\n",
        "            nn.BatchNorm1d(self.Encoder.model.config.hidden_size),\n",
        "            # nn.Dropout(data.get(\"drop\", 0.5)),\n",
        "            nn.Linear(self.Encoder.model.config.hidden_size, 200),\n",
        "            nn.Dropout(data.get(\"drop\", 0.5)),\n",
        "        )\n",
        "\n",
        "    # Classifier Layer\n",
        "    self.classifier = nn.Sequential(\n",
        "            # nn.LayerNorm(self.Encoder.model.config.hidden_size),\n",
        "            # nn.Dropout(data.get(\"drop\", 0.5)),\n",
        "            # nn.BatchNorm1d(self.Encoder.model.config.hidden_size),\n",
        "            # nn.Dropout(data.get(\"drop\", 0.5)),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(200, 1)\n",
        "        )\n",
        "\n",
        "    # Initializing layer parameters\n",
        "    nn.init.normal_(self.feature_map[1].weight, mean=0, std=0.00001)\n",
        "    nn.init.zeros_(self.feature_map[1].bias)\n",
        "\n",
        "  # Feed forward\n",
        "  def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
        "    \n",
        "    predict = self.Encoder(**{\"input_ids\":input_ids,\n",
        "                              \"attention_mask\":attention_mask,\n",
        "                              \"token_type_ids\":token_type_ids})\n",
        "    feature = self.feature_map(predict)\n",
        "    logit = self.classifier(feature)\n",
        "\n",
        "    predict = torch.sigmoid(logit)\n",
        "    \n",
        "    # Loss function \n",
        "    loss = self.loss_fn(logit.to(torch.float), labels.to(torch.float).unsqueeze(1))\n",
        "\n",
        "    return [loss, [feature, logit], predict]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta Learner"
      ],
      "metadata": {
        "id": "FwcGM27pSbDt"
      },
      "id": "FwcGM27pSbDt"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertForSequenceClassification\n",
        "from copy import deepcopy\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchmetrics\n",
        "from torchmetrics import functional as fn\n",
        "\n",
        "class Learner(nn.Module):\n",
        "\n",
        "    def __init__(self, **args):\n",
        "        \"\"\"\n",
        "        :param args:\n",
        "        \"\"\"\n",
        "        super(Learner, self).__init__()\n",
        "        \n",
        "        self.inner_print = args.get('inner_print')\n",
        "        self.inner_batch_size = args.get('inner_batch_size')\n",
        "        self.outer_update_lr  = args.get('outer_update_lr')\n",
        "        self.inner_update_lr  = args.get('inner_update_lr')\n",
        "        self.inner_update_step = args.get('inner_update_step')\n",
        "        self.inner_update_step_eval = args.get('inner_update_step_eval')\n",
        "        self.model = args.get('model')\n",
        "        self.device = args.get('device')\n",
        "        \n",
        "        # Outer optimizer\n",
        "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, batch_tasks, training = True, valid_train = True):\n",
        "        \"\"\"\n",
        "        batch = [(support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset)]\n",
        "        \n",
        "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
        "        \"\"\"\n",
        "        task_accs = []\n",
        "        task_f1 = []\n",
        "        task_recall = []\n",
        "        sum_gradients = []\n",
        "        num_task = len(batch_tasks)\n",
        "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
        "\n",
        "        # Outer loop tasks \n",
        "        for task_id, task in enumerate(batch_tasks):\n",
        "            support = task[0]\n",
        "            query   = task[1]\n",
        "            name   = task[2]\n",
        "            \n",
        "            # Copying model\n",
        "            fast_model = deepcopy(self.model)\n",
        "            fast_model.to(self.device)\n",
        "            \n",
        "            # Inner trainer optimizer\n",
        "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
        "            \n",
        "            # Creating training data loaders\n",
        "            if len(support) % self.inner_batch_size == 1 :\n",
        "              support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                              batch_size=self.inner_batch_size,\n",
        "                                              drop_last=True)\n",
        "            else:\n",
        "              support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                              batch_size=self.inner_batch_size,\n",
        "                                              drop_last=False)\n",
        "                            \n",
        "            # steps_per_epoch=len(support) // self.inner_batch_size\n",
        "            # total_training_steps = steps_per_epoch * 5\n",
        "            # warmup_steps = total_training_steps // 3\n",
        "            #            \n",
        "\n",
        "            # scheduler = get_linear_schedule_with_warmup(\n",
        "            #            inner_optimizer, \n",
        "            #           num_warmup_steps=warmup_steps,\n",
        "            #           num_training_steps=total_training_steps\n",
        "            #           )\n",
        "\n",
        "            fast_model.train()            \n",
        "\n",
        "            # Inner loop training epoch (support set)\n",
        "            if valid_train:\n",
        "              print('----Task',task_id,\":\", name, '----')\n",
        "\n",
        "            for i in range(0, num_inner_update_step):\n",
        "                all_loss = []\n",
        "\n",
        "                # Inner loop training batch (support set)\n",
        "                for inner_step, batch in enumerate(support_dataloader):\n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    input_ids, attention_mask, token_type_ids, label_id = batch\n",
        "\n",
        "                    # Feed Foward\n",
        "                    loss, _, _ = fast_model(input_ids, attention_mask, token_type_ids=token_type_ids, labels = label_id)\n",
        "                                  \n",
        "                    # Computing gradients\n",
        "                    loss.backward()\n",
        "                    # torch.nn.utils.clip_grad_norm_(fast_model.parameters(), max_norm=1)\n",
        "                    \n",
        "                    # Updating inner training parameters\n",
        "                    inner_optimizer.step()\n",
        "                    inner_optimizer.zero_grad()\n",
        "                    \n",
        "                    # Appending losses\n",
        "                    all_loss.append(loss.item())\n",
        "                    \n",
        "                    del batch, input_ids, attention_mask, label_id\n",
        "                    torch.cuda.empty_cache()\n",
        "                \n",
        "                if valid_train:\n",
        "                  if (i+1) % self.inner_print == 0:\n",
        "                      print(\"Inner Loss: \", np.mean(all_loss))\n",
        "\n",
        "            fast_model.to(torch.device('cpu'))\n",
        "            \n",
        "            # Inner training phase weights\n",
        "            if training:\n",
        "                meta_weights = list(self.model.parameters())\n",
        "                fast_weights = list(fast_model.parameters())\n",
        "\n",
        "                # Appending gradients\n",
        "                gradients = []\n",
        "                for i, (meta_params, fast_params) in enumerate(zip(meta_weights, fast_weights)):\n",
        "                    gradient = meta_params - fast_params\n",
        "                    if task_id == 0:\n",
        "                        sum_gradients.append(gradient)\n",
        "                    else:\n",
        "                        sum_gradients[i] += gradient\n",
        "\n",
        "\n",
        "            # Inner test (query set)\n",
        "            fast_model.to(self.device)\n",
        "            fast_model.eval()\n",
        "\n",
        "            if valid_train:\n",
        "              # Inner test (query set)\n",
        "              fast_model.to(self.device)\n",
        "              fast_model.eval()\n",
        "              \n",
        "            with torch.no_grad():\n",
        "                # Data loader\n",
        "                query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "                query_batch = iter(query_dataloader).next()\n",
        "                query_batch = tuple(t.to(self.device) for t in query_batch)\n",
        "                q_input_ids, q_attention_mask, q_token_type_ids, q_label_id = query_batch\n",
        "                \n",
        "                # Feedfoward\n",
        "                _, _, pre_label_id = fast_model(q_input_ids, q_attention_mask, q_token_type_ids, labels = q_label_id)\n",
        "\n",
        "                # Predictions\n",
        "                pre_label_id = pre_label_id.detach().cpu().squeeze()\n",
        "                # Labels\n",
        "                q_label_id = q_label_id.detach().cpu()\n",
        "\n",
        "                # Calculating metrics\n",
        "                acc = fn.accuracy(pre_label_id, q_label_id).item()\n",
        "                recall = fn.recall(pre_label_id, q_label_id).item(),\n",
        "                f1 = fn.f1_score(pre_label_id, q_label_id).item()\n",
        "\n",
        "                # appending metrics\n",
        "                task_accs.append(acc)\n",
        "                task_f1.append(f1)\n",
        "                task_recall.append(recall)\n",
        "            \n",
        "                fast_model.to(torch.device('cpu'))\n",
        "\n",
        "            del fast_model, inner_optimizer\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        print(\"\\n\")\n",
        "        print(\"f1:\",np.mean(task_f1))\n",
        "        print(\"recall:\",np.mean(task_recall))\n",
        "\n",
        "        # Updating outer training parameters\n",
        "        if training:\n",
        "            # Mean of gradients\n",
        "            for i in range(0,len(sum_gradients)):\n",
        "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
        "\n",
        "            # Indexing parameters to model\n",
        "            for i, params in enumerate(self.model.parameters()):\n",
        "                params.grad = sum_gradients[i]\n",
        "\n",
        "            # Updating parameters\n",
        "            self.outer_optimizer.step()\n",
        "            self.outer_optimizer.zero_grad()\n",
        "            \n",
        "            del sum_gradients\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        if valid_train:\n",
        "          return np.mean(task_accs)\n",
        "        else:\n",
        "          return np.array(0)"
      ],
      "metadata": {
        "id": "vub8I7nUyFwz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vub8I7nUyFwz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task Loader\n"
      ],
      "metadata": {
        "id": "lZIKqVG6HpoS"
      },
      "id": "lZIKqVG6HpoS"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "LABEL_MAP = {'negative': 0, 'positive': 1}\n",
        "\n",
        "class SLR_DataSet(Dataset):\n",
        "  def __init__(self, **args):\n",
        "    self.tokenizer = args.get('tokenizer')\n",
        "    self.data = args.get('data')\n",
        "    self.max_seq_length = args.get(\"max_seq_length\", 512)\n",
        "    self.INPUT_NAME = args.get(\"input\", 'x')\n",
        "    self.LABEL_NAME = args.get(\"output\", 'y')\n",
        "\n",
        "  # Tokenizing and processing text\n",
        "  def encode_text(self, example):\n",
        "    comment_text = example[self.INPUT_NAME]\n",
        "    comment_text = self.treat_text(comment_text)\n",
        "    \n",
        "    labels = LABEL_MAP[example[self.LABEL_NAME]]\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      (comment_text, \"It is great text\"),\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_seq_length,\n",
        "      return_token_type_ids=True,\n",
        "      padding=\"max_length\",\n",
        "      truncation=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    \n",
        "    return tuple((\n",
        "      encoding[\"input_ids\"].flatten(),\n",
        "      encoding[\"attention_mask\"].flatten(),\n",
        "      encoding[\"token_type_ids\"].flatten(),\n",
        "      torch.tensor([torch.tensor(labels).to(int)])\n",
        "    ))\n",
        "  \n",
        "  # Text processing function\n",
        "  def treat_text(self, text):\n",
        "    text = unicodedata.normalize(\"NFKD\",str(text))\n",
        "    text = multiple_replace(patterns,text.lower())\n",
        "    text = re.sub('(\\(.+\\))|(\\[.+\\])|( \\d )|(<)|(>)|(- )','', text)\n",
        "    text = re.sub('( +)',' ', text)\n",
        "    text = re.sub('(, ,)|(,,)',',', text)\n",
        "    text = re.sub('(%)|(per cent)',' percent', text)\n",
        "    return text\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  # Returning data\n",
        "  def __getitem__(self, index: int):\n",
        "    # print(index)\n",
        "    data_row = self.data.reset_index().iloc[index]\n",
        "    temp_data =  self.encode_text(data_row)\n",
        "    return temp_data"
      ],
      "metadata": {
        "id": "CPY4pl1kPFMV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "CPY4pl1kPFMV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks maker"
      ],
      "metadata": {
        "id": "zCzWI2YKMoks"
      },
      "id": "zCzWI2YKMoks"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "LABEL_MAP  = {'positive':1, 'negative':0}\n",
        "\n",
        "# Creating Meta Tasks\n",
        "class MetaTask(Dataset):\n",
        "    def __init__(self, examples, num_task, k_support, k_query, tokenizer, training=True, max_seq_length=512, **args):\n",
        "        \"\"\"\n",
        "        :param samples: list of samples\n",
        "        :param num_task: number of training tasks.\n",
        "        :param k_support: number of classes support samples per task\n",
        "        :param k_query: number of classes query sample per task\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        \n",
        "        self.num_task =  num_task\n",
        "        self.k_support = k_support\n",
        "        self.k_query = k_query\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "        # Randomly generating tasks\n",
        "        self.create_batch(self.num_task, training)\n",
        "        \n",
        "    # Creating batch\n",
        "    def create_batch(self, num_task, training):\n",
        "        self.supports = []  # support set\n",
        "        self.queries = []  # query set\n",
        "        self.task_names = [] # Name of task\n",
        "        self.supports_indexs = [] # index of supports\n",
        "        self.queries_indexs = [] # index of queries\n",
        "        self.num_task=num_task\n",
        "        \n",
        "        # Available tasks\n",
        "        domains = self.examples['domain'].unique()\n",
        "\n",
        "        # If not training, create all tasks\n",
        "        if not(training):\n",
        "          self.task_names = domains\n",
        "          num_task = len(self.task_names)\n",
        "          self.num_task=num_task\n",
        "\n",
        "        \n",
        "        for b in range(num_task):  # For each task,\n",
        "            total_per_class = self.k_support + self.k_query \n",
        "            task_size = 2*self.k_support + 2*self.k_query \n",
        "\n",
        "            # Select a task at random\n",
        "            if training:  \n",
        "              domain = random.choice(domains)\n",
        "              self.task_names.append(domain)\n",
        "            else:\n",
        "              domain = self.task_names[b]\n",
        "\n",
        "            # Task data\n",
        "            domainExamples = self.examples[self.examples['domain'] == domain]\n",
        "\n",
        "            # Minimal label quantity\n",
        "            min_per_class = min(domainExamples['label'].value_counts())\n",
        "\n",
        "            if total_per_class > min_per_class:\n",
        "              total_per_class = min_per_class\n",
        "            \n",
        "            # Select k_support + k_query task examples\n",
        "            # Sample (n) from each label(class)\n",
        "            selected_examples = domainExamples.groupby(\"label\").sample(total_per_class, replace = False)\n",
        "\n",
        "            # Split data into support (training) and query (testing) sets\n",
        "            s, q = train_test_split(selected_examples,\n",
        "                                    stratify= selected_examples[\"label\"],\n",
        "                                    test_size= 2*self.k_query/task_size,\n",
        "                                    shuffle=True)\n",
        "            \n",
        "            # Permutating data\n",
        "            s = s.sample(frac=1)  \n",
        "            q = q.sample(frac=1) \n",
        "\n",
        "            # Appending indexes\n",
        "            if not(training):\n",
        "              self.supports_indexs.append(s.index)\n",
        "              self.queries_indexs.append(q.index)\n",
        "\n",
        "            # Creating list of support (training) and query (testing) tasks\n",
        "            self.supports.append(s.to_dict('records'))\n",
        "            self.queries.append(q.to_dict('records'))\n",
        "\n",
        "    # Creating task tensors\n",
        "    def create_feature_set(self, examples):\n",
        "        all_input_ids      = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_attention_mask = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_token_type_ids = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
        "\n",
        "        for _id, e in enumerate(examples):\n",
        "          all_input_ids[_id], all_attention_mask[_id], all_token_type_ids[_id], all_label_ids[_id] = self.encode_text(e)\n",
        "\n",
        "        return TensorDataset(\n",
        "            all_input_ids,\n",
        "            all_attention_mask,\n",
        "            all_token_type_ids,\n",
        "            all_label_ids\n",
        "        ) \n",
        "      \n",
        "    # Data encoding\n",
        "    def encode_text(self, example):\n",
        "      comment_text = example[\"text\"]\n",
        "      comment_text = self.treat_text(comment_text)\n",
        "      labels = LABEL_MAP[example[\"label\"]]\n",
        "\n",
        "      encoding = self.tokenizer.encode_plus(\n",
        "        (comment_text, \"It is great text\"),\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_seq_length,\n",
        "        return_token_type_ids=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "      )\n",
        "\n",
        "      return tuple((\n",
        "        encoding[\"input_ids\"].flatten(),\n",
        "        encoding[\"attention_mask\"].flatten(),\n",
        "        encoding[\"token_type_ids\"].flatten(),\n",
        "        torch.tensor([torch.tensor(labels).to(int)])\n",
        "      ))\n",
        "\n",
        "\n",
        "    # Regex text processing\n",
        "    def treat_text(self, text):\n",
        "      text = unicodedata.normalize(\"NFKD\",str(text))\n",
        "      text = multiple_replace(patterns,text.lower())\n",
        "      text = re.sub('(\\(.+\\))|(\\[.+\\])|( \\d )|(<)|(>)|(- )','', text)\n",
        "      text = re.sub('( +)',' ', text)\n",
        "      text = re.sub('(, ,)|(,,)',',', text)\n",
        "      text = re.sub('(%)|(per cent)',' percent', text)\n",
        "      return text\n",
        "\n",
        "    # Returns data upon calling\n",
        "    def __getitem__(self, index):\n",
        "        support_set = self.create_feature_set(self.supports[index])\n",
        "        query_set   = self.create_feature_set(self.queries[index])\n",
        "        name        = self.task_names[index]\n",
        "        return support_set, query_set, name\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_task"
      ],
      "metadata": {
        "id": "tLSwhmNu0sKX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tLSwhmNu0sKX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50-50 split simulation"
      ],
      "metadata": {
        "id": "v8TU8TxgEkIL"
      },
      "id": "v8TU8TxgEkIL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "3tWCZeIM2zHV"
      },
      "id": "3tWCZeIM2zHV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Statistics"
      ],
      "metadata": {
        "id": "jJwL3bIiIpIp"
      },
      "id": "jJwL3bIiIpIp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading descriptive statistics\n",
        "info_load = load_data_statistics(paths)\n",
        "\n",
        "info_load.head()"
      ],
      "metadata": {
        "id": "H7xiljcd6DXu"
      },
      "id": "H7xiljcd6DXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data split"
      ],
      "metadata": {
        "id": "ypcWFBRNFAt7"
      },
      "id": "ypcWFBRNFAt7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining selection criteria for the training dataset: datasets with over 50 positive and 50 negative labels\n",
        "select_criteria = (info_load['pos'] >= 50) & (info_load['neg'] >= 50)\n",
        "print(info_load[select_criteria]['names'] )"
      ],
      "metadata": {
        "id": "Q8_rBa87K4ke"
      },
      "id": "Q8_rBa87K4ke",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the training dataset\n",
        "Train_resource = load_data(\n",
        "    info_load[select_criteria]\n",
        ")\n",
        "\n",
        "Train_resource = Train_resource.dropna(axis=0).reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "# Validation data\n",
        "conditional = Train_resource['domain'] == \"PFOS-PFOA.csv\"\n",
        "valid_tranf = Train_resource[conditional]\n",
        "Train_resource = Train_resource[~conditional].reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "cgu3efPhRgMc"
      },
      "id": "cgu3efPhRgMc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting data with at least 40 words separated by spaces\n",
        "crit = Train_resource[\"text\"].apply(lambda x: len(x.split(\" \"))) >= 40\n",
        "Train_resource = Train_resource[crit].reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "mf0-FmmVRhp8"
      },
      "id": "mf0-FmmVRhp8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data that doesn't fit 50pos/50neg criteria\n",
        "rest_resource = load_data(\n",
        "    info_load[~select_criteria]\n",
        ")\n",
        "\n",
        "rest_resource = pd.concat([valid_tranf, rest_resource]).reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "01uw7iFBU9dk"
      },
      "id": "01uw7iFBU9dk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the testing dataset\n",
        "Test_resource = rest_resource[rest_resource['domain'].isin(['SR11_Li.csv',\n",
        " 'SR14_Funakoshi.csv',\n",
        " 'SR2_Meng.csv',\n",
        " 'SR6_Wang.csv',\n",
        " 'SR7_Zhou.csv',\n",
        " 'SR8_Liu.csv',\n",
        " 'SR9_Douxfils.csv',\n",
        " 'Distal_radius_fractures_approach.csv',\n",
        " 'Hallux_valgus_prognostic.csv',\n",
        " 'Head_and_neck_cancer_imaging.csv',\n",
        " 'Obstetric_emergency_training.csv',\n",
        " 'Pregnancy_medication.csv',\n",
        " 'Shoulderdystocia_positioning.csv',\n",
        " 'Shoulderdystocia_recurrence.csv',\n",
        " 'SR12_Cavender.csv',\n",
        " 'SR13_Chatterjee.csv',\n",
        " 'SR1_Yang.csv',\n",
        " 'SR3_Segelov.csv',\n",
        " 'SR4_Li.csv',\n",
        " 'SR5_Lv.csv'])]\n",
        "\n",
        "Test_resource = Test_resource.dropna(axis=0).reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "# Selecting data with at least 40 words separated by spaces\n",
        "crit = Test_resource[\"text\"].apply(lambda x: len(x.split(\" \"))) >= 40\n",
        "Test_resource = Test_resource[crit].reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "frk2kNQgnSjy"
      },
      "id": "frk2kNQgnSjy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the validation dataset\n",
        "Valid_resource = rest_resource[rest_resource['domain'].isin(['PFOS-PFOA.csv',\n",
        " 'Bos_2018.csv',\n",
        " 'Wolters_2018.csv',\n",
        " 'SkeletalMuscleRelaxants.csv',\n",
        " 'Fluoride.csv',\n",
        " 'Kitchenham_2010.csv',\n",
        " 'Radjenovic_2013.csv',\n",
        " 'Opiods.csv',\n",
        " 'Leafy_Greens_Future_set.csv',\n",
        " 'Distal_radius_fractures_closed_reduction.csv',\n",
        " 'Head_and_neck_cancer_bone.csv',\n",
        " 'Shoulder_replacement_diagnostic.csv',\n",
        " 'Shoulder_replacement_surgery.csv',\n",
        " 'Total_knee_replacement.csv',\n",
        " 'Vascular_access.csv'])]\n",
        "\n",
        "Valid_resource =  Valid_resource.dropna(axis=0).reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "# Selecting data with at least 40 words separated by spaces\n",
        "crit = Valid_resource[\"text\"].apply(lambda x: len(x.split(\" \"))) >= 40\n",
        "Valid_resource = Valid_resource[crit].reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "BWUeVk1NU6zV"
      },
      "id": "BWUeVk1NU6zV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "lqDo8-v2FODY"
      },
      "id": "lqDo8-v2FODY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training dataset"
      ],
      "metadata": {
        "id": "Wh2XVuYGmo4v"
      },
      "id": "Wh2XVuYGmo4v"
    },
    {
      "cell_type": "code",
      "source": [
        "Train_resource[\"text\"].apply(lambda x: len(x.split(\" \"))).hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H1CyN8RxOlEG"
      },
      "id": "H1CyN8RxOlEG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_resource.groupby('domain')['label']\\\n",
        "              .value_counts()"
      ],
      "metadata": {
        "id": "26iDqWZp7kFC"
      },
      "id": "26iDqWZp7kFC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing Dataset"
      ],
      "metadata": {
        "id": "9SYTNYLQqWPG"
      },
      "id": "9SYTNYLQqWPG"
    },
    {
      "cell_type": "code",
      "source": [
        "Test_resource[\"text\"].apply(lambda x: len(x.split(\" \"))).hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a-aR0DKVOtLm"
      },
      "id": "a-aR0DKVOtLm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test_resource.groupby('domain')['label']\\\n",
        "              .value_counts()"
      ],
      "metadata": {
        "id": "DhitaMNdmMiE"
      },
      "id": "DhitaMNdmMiE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation Dataset"
      ],
      "metadata": {
        "id": "4S0AvO8AqjV3"
      },
      "id": "4S0AvO8AqjV3"
    },
    {
      "cell_type": "code",
      "source": [
        "Valid_resource[\"text\"].apply(lambda x: len(x.split(\" \"))).hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zKHgSV2wO291"
      },
      "id": "zKHgSV2wO291",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Valid_resource.groupby('domain')['label']\\\n",
        "              .value_counts()"
      ],
      "metadata": {
        "id": "_iCLwIZ9mMxf"
      },
      "id": "_iCLwIZ9mMxf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta learning phase"
      ],
      "metadata": {
        "id": "e0nquuSXdL40"
      },
      "id": "e0nquuSXdL40"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Meta-Training informations\n",
        "Info = {\n",
        "    \"inner_print\": 2, \n",
        "    \"bert_layers\": 2,\n",
        "    \"tokenizer\": initializer_model.tokenizer,\n",
        "    \"max_seq_length\": 512,\n",
        "    \"meta_epoch\":5, # Outer loop epochs\n",
        "    \"k_spt\":8, # Support (training) examples per class, binary case\n",
        "    \"k_qry\":8, # Query (testing) examples per class, binary case\n",
        "    \"outer_batch_size\": 5, # Size of batch of tasks\n",
        "    \"inner_batch_size\": 4, # Size of batch of classifications\n",
        "    \"outer_update_lr\" : 5e-5, # Learning rate of task optimizer\n",
        "    \"inner_update_lr\" : 5e-5, # Learning rate of classification optimizer\n",
        "    \"inner_update_step\" : 4, # Inner loop epochs\n",
        "    \"inner_update_step_eval\": 4, # Validation inner loop epochs\n",
        "    \"num_task_train\" : 2, # Number of training tasks\n",
        "    # \"num_task_test\" : 5 # Number of testing tasks\n",
        "    \"pos_weight\" : 3 # p > 1 increases recall, p < 1 increases precision, applied in loss function\n",
        "}\n",
        "\n",
        "model = SLR_Classifier(bert_layers = range(Info[\"bert_layers\"]),\n",
        "                       model = initializer_model.model.bert,\n",
        "                       drop=0.2)\n",
        "\n",
        "\n",
        "meta_train(data = Train_resource,\n",
        "          model = model,\n",
        "          device = device,\n",
        "          Info = Info,\n",
        "          print_epoch =True,\n",
        "          size_layer=Info[\"bert_layers\"],\n",
        "          Test_resource=Test_resource)"
      ],
      "metadata": {
        "id": "4LK6jCM4eNiu"
      },
      "id": "4LK6jCM4eNiu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing trained meta model"
      ],
      "metadata": {
        "id": "w9V-_m7Ne49G"
      },
      "id": "w9V-_m7Ne49G"
    },
    {
      "cell_type": "code",
      "source": [
        "Valid_resource.groupby('domain')['label'].value_counts()"
      ],
      "metadata": {
        "id": "XNlkp4iRdEeO"
      },
      "id": "XNlkp4iRdEeO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task diagnosis"
      ],
      "metadata": {
        "id": "zYEnZZLqHyb-"
      },
      "id": "zYEnZZLqHyb-"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "# Initializing model\n",
        "model_to_finetunning = deepcopy(model)\n",
        "\n",
        "# Creating validation tasks\n",
        "valid = MetaTask(Valid_resource,\n",
        "                 num_task = 500,\n",
        "                 k_support=10,\n",
        "                 k_query=20,\n",
        "                 tokenizer = Info['tokenizer'],\n",
        "                 training=False)\n",
        "\n",
        "# Diagnostic dataset\n",
        "i = valid.task_names =='Opiods.csv'\n",
        "\n",
        "# Task index\n",
        "idx = np.array(range(len(i)))[i].item()\n",
        "\n",
        "# Support (train) e query (test) data\n",
        "support = valid[idx][0]\n",
        "query   = valid[idx][1]\n",
        "name   = valid[idx][2]\n",
        "\n",
        "print(name)\n",
        "print(Valid_resource[Valid_resource['domain']== name]['label'].value_counts())\n",
        "print(\"k_suport_examples:\",len(support))\n",
        "print(\"k_query_examples:\",len(query))\n",
        "\n",
        "# Support data loader\n",
        "support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                batch_size=5)\n"
      ],
      "metadata": {
        "id": "ikoenNhwJ85P"
      },
      "id": "ikoenNhwJ85P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Untrained model"
      ],
      "metadata": {
        "id": "oiBSWSB2dUh4"
      },
      "id": "oiBSWSB2dUh4"
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_finetunning.to(device)\n",
        "\n",
        "# Inner Optimizer\n",
        "inner_optimizer = Adam(model_to_finetunning.parameters(), lr=5e-5)\n",
        "\n",
        "\n",
        "# Predicting\n",
        "model_to_finetunning.eval()\n",
        "with torch.no_grad():\n",
        "    query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "    query_batch = iter(query_dataloader).next()\n",
        "    query_batch = tuple(t.to(device) for t in query_batch)\n",
        "    q_input_ids, q_attention_mask, q_token_type_ids, q_label_id = query_batch\n",
        "    \n",
        "    # Predictions\n",
        "    _, features, predictions = model_to_finetunning(q_input_ids, q_attention_mask, q_token_type_ids, labels = q_label_id)\n",
        "\n",
        "    predictions = predictions.detach().cpu().squeeze()\n",
        "    q_label_id = q_label_id.detach().cpu()\n",
        "\n",
        "    acc = fn.accuracy(predictions, q_label_id).item()\n",
        "    print(\"acc:\",acc)\n",
        "\n",
        "# TSNE Dimensionality reduction\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(features[0].to('cpu'))\n",
        "\n",
        "# Plot\n",
        "sns.scatterplot(x=X_embedded[:, 0],\n",
        "                y=X_embedded[:, 1],\n",
        "                hue=q_label_id)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jB22A0rOJ_uD"
      },
      "id": "jB22A0rOJ_uD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trained model"
      ],
      "metadata": {
        "id": "QKoI4C-zeOLM"
      },
      "id": "QKoI4C-zeOLM"
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_finetunning = deepcopy(model)\n",
        "inner_optimizer = Adam(model_to_finetunning.parameters(), lr=5e-5)\n",
        "model_to_finetunning.train()\n",
        "model_to_finetunning.to(device)\n",
        "\n",
        "# Inner loop training\n",
        "for i in range(0,Info['inner_update_step']):\n",
        "    all_loss = []\n",
        "\n",
        "    # Inner training batch (support set)\n",
        "    for inner_step, batch in enumerate(support_dataloader):\n",
        "        \n",
        "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
        "        input_ids, attention_mask, token_type_ids, label_id = batch\n",
        "\n",
        "        # Feed Foward\n",
        "        loss, _, _ = model_to_finetunning(input_ids, attention_mask, token_type_ids=token_type_ids, labels = label_id)\n",
        "                      \n",
        "        loss.backward()\n",
        "        inner_optimizer.step()\n",
        "        inner_optimizer.zero_grad()\n",
        "        \n",
        "        all_loss.append(loss.item())\n",
        "    \n",
        "    if i % Info[\"inner_print\"] == 0:\n",
        "        print(\"Inner Loss: \", np.mean(all_loss))\n",
        "\n",
        "# Predicting\n",
        "model_to_finetunning.eval()\n",
        "with torch.no_grad():\n",
        "    query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "    query_batch = iter(query_dataloader).next()\n",
        "    query_batch = tuple(t.to(device) for t in query_batch)\n",
        "    q_input_ids, q_attention_mask, q_token_type_ids, q_label_id = query_batch\n",
        "    \n",
        "    # Predictions\n",
        "    _, features, predictions = model_to_finetunning(q_input_ids, q_attention_mask, q_token_type_ids, labels = q_label_id)\n",
        "\n",
        "    predictions = predictions.detach().cpu().squeeze()\n",
        "    q_label_id = q_label_id.detach().cpu()\n",
        "\n",
        "    acc = fn.accuracy(predictions, q_label_id).item()\n",
        "    print(\"acc:\",acc)\n",
        "\n",
        "\n",
        "\n",
        "model_to_finetunning.to(torch.device('cpu'))\n",
        "del  inner_optimizer, model_to_finetunning\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# TSNE Dimensionality reduction\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(features[0].to('cpu'))\n",
        "\n",
        "# Plot\n",
        "sns.scatterplot(x=X_embedded[:, 0],\n",
        "                y=X_embedded[:, 1],\n",
        "                hue=q_label_id,\n",
        "                alpha=torch.sigmoid(features[1]).to('cpu').view(-1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UTw968K405Yn"
      },
      "id": "UTw968K405Yn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain learning phase with validation resource"
      ],
      "metadata": {
        "id": "aXVhqbScO0GB"
      },
      "id": "aXVhqbScO0GB"
    },
    {
      "cell_type": "code",
      "source": [
        "Valid_resource.groupby('domain')['label'].value_counts()"
      ],
      "metadata": {
        "id": "tl5C-7f5-Zbm"
      },
      "id": "tl5C-7f5-Zbm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diagnosis"
      ],
      "metadata": {
        "id": "CmXXb4wGglSs"
      },
      "id": "CmXXb4wGglSs"
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "# Info = {\n",
        "#     \"inner_print\": 1, \n",
        "#     \"bert_layers\": 3,\n",
        "#     \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "#     # \"model\": model,\n",
        "#     \"tokenizer\": initializer_bert.tokenizer,\n",
        "#     \"max_seq_length\": 512,\n",
        "#     \"meta_epoch\":30, # Numero de epocas do outerloop\n",
        "#     \"k_spt\":20, # Numero de treino por classe (k_spt) caso binario\n",
        "#     \"k_qry\":20, # Numero de valicao por classe (k_spt) caso binario\n",
        "#     \"outer_batch_size\": 5, # Divide as tasks em batchs tasks\n",
        "#     \"inner_batch_size\": 5, # Divide as classificacao em batch de classificao\n",
        "#     \"outer_update_lr\" : 5e-5, # Learning rate do otimizador das tasks\n",
        "#     \"inner_update_lr\" : 5e-5, # Learning rate do otimizador das classificao\n",
        "#     \"inner_update_step\" : 3, # Numero de epocas dentro do inner loop\n",
        "#     \"inner_update_step_eval\": 3, # Numero de epocas dentro do inner loop de validacao\n",
        "#     \"num_task_train\" : 100, # Quantidade de tarefas de treino\n",
        "#     \"num_task_test\" : 5 # Quantidade de tarefas de test\n",
        "# }\n",
        "\n",
        "Info['tresh'] = 0.9\n",
        "Info[\"inner_update_step_eval\"] = 3\n",
        "Info[\"inner_print\"] = 1"
      ],
      "metadata": {
        "id": "j7MZtpEjIeeH"
      },
      "id": "j7MZtpEjIeeH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful information:\n",
        "\n",
        "+ Variables should be specified within `Info`\n",
        "\n",
        "+ Model should be in the `model` variable\n",
        "\n",
        "+ Training should be done with specification in the `device` variable\n",
        "\n",
        "It's possible to access dome of the data from the testing dataset after pressing \"Train\", such as:\n",
        "\n",
        "+ `logits`: logits from classifier stage, no activation\n",
        "\n",
        "+ `X_embedded`: 2-dimensional values from dimensionality reduction of the latent space\n",
        "\n",
        "+ `features`: Latent space values  (feature_map layer output)\n",
        "\n",
        "+ `labels`: True values\n",
        "\n",
        "+ `data_train`: Training data (Not the same order given to the model on training)\n",
        "\n",
        "+ `data_test`: Testing data\n",
        "\n",
        "+ `batch_size_test`: Batch size from the testing dataset, so that prediction time can be reduced"
      ],
      "metadata": {
        "id": "8wJDcXqCkwFC"
      },
      "id": "8wJDcXqCkwFC"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluating validation dataset\n",
        "batch_size_test =   20 #@param {type:\"number\"}\n",
        "# Task names\n",
        "names = Valid_resource['domain'].unique()\n",
        "\n",
        "diagnosis5050 = diagnosis(names, Valid_resource, batch_size_test, model,Info, start=0)\n",
        "diagnosis5050()"
      ],
      "metadata": {
        "id": "HySCP9L69uZn"
      },
      "id": "HySCP9L69uZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Avaliation of Positives examples"
      ],
      "metadata": {
        "id": "2Uqfxt_6eq2h"
      },
      "id": "2Uqfxt_6eq2h"
    },
    {
      "cell_type": "code",
      "source": [
        "def treat_text(text):\n",
        "    text = unicodedata.normalize(\"NFKD\",str(text))\n",
        "    text = multiple_replace(patterns,text.lower())\n",
        "    text = re.sub('(\\(.+\\))|(\\[.+\\])|( \\d )|(<)|(>)|(- )','', text)\n",
        "    text = re.sub('( +)',' ', text)\n",
        "    text = re.sub('(, ,)|(,,)',',', text)\n",
        "    text = re.sub('(%)|(per cent)',' percent', text)\n",
        "    return text\n",
        "\n",
        "data_test = diagnosis5050.data_test\n",
        "logits = diagnosis5050.logits\n",
        "\n",
        "# random choice a positive example\n",
        "indx= data_test[data_test['label'] == 'positive'].index\n",
        "indx= np.random.choice(indx)\n",
        "\n",
        "\n",
        "# print the example\n",
        "pprint(data_test.iloc[indx])\n",
        "\n",
        "# Prediction by the model\n",
        "print(\"Predicted:\",torch.sigmoid(logits[indx]).item() )\n",
        "\n",
        "# Text of the example\n",
        "print('Treated Text:')\n",
        "pprint(treat_text(data_test['text'].iloc[indx]))\n",
        "\n",
        "# Text of the example\n",
        "print('Text:')\n",
        "pprint(treat_text(data_test['text'].iloc[indx]))"
      ],
      "metadata": {
        "id": "Mj0DnVZivqp7"
      },
      "id": "Mj0DnVZivqp7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "gsJ2j0f4MH5P"
      },
      "id": "gsJ2j0f4MH5P"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import datetime\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "base_path = '5050split'\n",
        "path_save = \"./\"+\"Models/\"+base_path+\"/\"\n",
        "\n",
        "\n",
        "# Creating directory\n",
        "Path(path_save).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Directory data\n",
        "check = (str(datetime.datetime.now()))[0:19]\n",
        "check = re.sub(\"[:-]\",\"_\",check)\n",
        "check = re.sub(\" \",\"_hr_\",check)\n",
        "\n",
        "print(\"Data e hora do salvamento:\", check)\n",
        "\n",
        "# Creating directory\n",
        "Path(f\"{path_save}/{check}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Saving paths\n",
        "model_path = f'{path_save}/{check}/model.pt'\n",
        "meta_info_path = f'{path_save}/{check}/Info.json'\n",
        "\n",
        "# Meta info\n",
        "save_info = Info.copy()\n",
        "save_info['model'] = initializer_model.tokenizer.name_or_path\n",
        "save_info.pop(\"tokenizer\")\n",
        "\n",
        "## Saving meta info\n",
        "with open(meta_info_path, 'w') as fp:\n",
        "    json.dump(save_info, fp)\n",
        "\n",
        "## Saving entire model\n",
        "torch.save(model, model_path)\n",
        "\n",
        "# To save only parameters:\n",
        "# torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "eUBZ0xQvMHR5"
      },
      "id": "eUBZ0xQvMHR5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "Qz8B6OOIbkW1"
      },
      "id": "Qz8B6OOIbkW1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading entire model\n",
        "model = torch.load(model_path)\n",
        "model"
      ],
      "metadata": {
        "id": "HYn3Rwvvr0Sj"
      },
      "id": "HYn3Rwvvr0Sj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing simulation attemps"
      ],
      "metadata": {
        "id": "mShjJAP5qFuh"
      },
      "id": "mShjJAP5qFuh"
    },
    {
      "cell_type": "code",
      "source": [
        "names_to_valid = Valid_resource['domain'].unique()"
      ],
      "metadata": {
        "id": "rjWkAFtbrWk5"
      },
      "id": "rjWkAFtbrWk5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.clear_autocast_cache()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "base_path = '5050split'\n",
        "path_save = \"./\"+\"Results/\"+base_path+\"/\"\n",
        "\n",
        "pipeline_simpulation(Valid_resource, names_to_valid, path_save, model, Info)"
      ],
      "metadata": {
        "id": "LyEra68ZrBFR"
      },
      "id": "LyEra68ZrBFR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison simulation"
      ],
      "metadata": {
        "id": "UHyBqbJNwzXA"
      },
      "id": "UHyBqbJNwzXA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data"
      ],
      "metadata": {
        "id": "d_Jp8EaAwzXB"
      },
      "id": "d_Jp8EaAwzXB"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "path = 'SLR_data'\n",
        "\n",
        "cohen_paths = glob.glob(f\"{path}/**/cohen/*.csv\", recursive=True)\n",
        "SWIFT_paths = glob.glob(f\"{path}/**/SWIFT systematic review data/*.csv\", recursive=True)\n",
        "\n",
        "cohen_names = [os.path.basename(p) for p in cohen_paths]\n",
        "SWIFT_names = [os.path.basename(p) for p in SWIFT_paths]\n",
        "\n",
        "base_path = 'Comparison'"
      ],
      "metadata": {
        "id": "4vSHwaa7xZ1P"
      },
      "id": "4vSHwaa7xZ1P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Statistics"
      ],
      "metadata": {
        "id": "sxq52iO6wzXB"
      },
      "id": "sxq52iO6wzXB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data split"
      ],
      "metadata": {
        "id": "fv1TvfW-wzXC"
      },
      "id": "fv1TvfW-wzXC"
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "All_data = load_data(\n",
        "    info_load\n",
        ")\n",
        "\n",
        "All_data = All_data.dropna().reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "comparison_names = list(chain.from_iterable([cohen_names, SWIFT_names]))\n",
        "\n",
        "cohen_data = All_data[All_data['domain'].isin(cohen_names)]\n",
        "cohen_data =  cohen_data.reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "SWIFT_data = All_data[All_data['domain'].isin(SWIFT_names)]\n",
        "SWIFT_data =  SWIFT_data.reset_index().drop(\"index\", axis=1)\n",
        "\n",
        "Train_resource = All_data[~All_data['domain'].isin(comparison_names)]\n",
        "Train_resource =  Train_resource.reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "Nc_73DYFyyDn"
      },
      "id": "Nc_73DYFyyDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting data with at least 40 words separated by spaces\n",
        "crit = Train_resource[\"text\"].apply(lambda x: len(x.split(\" \"))) >= 40\n",
        "Train_resource = Train_resource[crit].reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "LdGqKwZ4wzXC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "LdGqKwZ4wzXC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "IM4SBF4CwzXD"
      },
      "id": "IM4SBF4CwzXD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training dataset"
      ],
      "metadata": {
        "id": "zE5FeDmUwzXD"
      },
      "id": "zE5FeDmUwzXD"
    },
    {
      "cell_type": "code",
      "source": [
        "Train_resource[\"text\"].apply(lambda x: len(x.split(\" \"))).hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6RduBiDVwzXE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "6RduBiDVwzXE"
    },
    {
      "cell_type": "code",
      "source": [
        "Train_resource.groupby('domain')['label']\\\n",
        "              .value_counts()"
      ],
      "metadata": {
        "id": "PT03s--fwzXE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "PT03s--fwzXE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### cohen Dataset"
      ],
      "metadata": {
        "id": "GplApKOfwzXE"
      },
      "id": "GplApKOfwzXE"
    },
    {
      "cell_type": "code",
      "source": [
        "cohen_data[\"text\"].apply(lambda x: len(x.split(\" \"))).hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5p1tAYcSwzXE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5p1tAYcSwzXE"
    },
    {
      "cell_type": "code",
      "source": [
        "cohen_data.groupby('domain')['label']\\\n",
        "              .value_counts()"
      ],
      "metadata": {
        "id": "qPCW62_XwzXE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qPCW62_XwzXE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SWIFT Dataset"
      ],
      "metadata": {
        "id": "YXLQORwJwzXE"
      },
      "id": "YXLQORwJwzXE"
    },
    {
      "cell_type": "code",
      "source": [
        "SWIFT_data[\"text\"].apply(lambda x: len(x.split(\" \"))).hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yegMKoehwzXE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yegMKoehwzXE"
    },
    {
      "cell_type": "code",
      "source": [
        "SWIFT_data.groupby('domain')['label']\\\n",
        "              .value_counts()"
      ],
      "metadata": {
        "id": "T6qSFORtwzXE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "T6qSFORtwzXE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meta learning phase"
      ],
      "metadata": {
        "id": "YFxdu9b2wzXE"
      },
      "id": "YFxdu9b2wzXE"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Meta-Training informations\n",
        "Info = {\n",
        "    \"inner_print\": 2, \n",
        "    \"bert_layers\": 2,\n",
        "    \"tokenizer\": initializer_model.tokenizer,\n",
        "    \"max_seq_length\": 512,\n",
        "    \"meta_epoch\":5, # Outer loop epochs\n",
        "    \"k_spt\":8, # Support (training) examples per class, binary case\n",
        "    \"k_qry\":8, # Query (testing) examples per class, binary case\n",
        "    \"outer_batch_size\": 5, # Size of batch of tasks\n",
        "    \"inner_batch_size\": 4, # Size of batch of classifications\n",
        "    \"outer_update_lr\" : 5e-5, # Learning rate of task optimizer\n",
        "    \"inner_update_lr\" : 5e-5, # Learning rate of classification optimizer\n",
        "    \"inner_update_step\" : 4, # Inner loop epochs\n",
        "    \"inner_update_step_eval\": 4, # Validation inner loop epochs\n",
        "    \"num_task_train\" : 2, # Number of training tasks\n",
        "    # \"num_task_test\" : 5 # Number of testing tasks\n",
        "    \"pos_weight\" : 3 # p > 1 increases recall, p < 1 increases precision, applied in loss function\n",
        "}\n",
        "\n",
        "model = SLR_Classifier(bert_layers = range(Info[\"bert_layers\"]),\n",
        "                       model = initializer_model.model.bert,\n",
        "                       drop=0.2)\n",
        "\n",
        "\n",
        "meta_train(data = Train_resource,\n",
        "          model = model,\n",
        "          device = device,\n",
        "          Info = Info,\n",
        "          print_epoch =True,\n",
        "          size_layer=Info[\"bert_layers\"],\n",
        "          Test_resource=None)"
      ],
      "metadata": {
        "id": "4RZcNhCawzXE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4RZcNhCawzXE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing trained meta model in SWIFT"
      ],
      "metadata": {
        "id": "6PoUx5zCwzXE"
      },
      "id": "6PoUx5zCwzXE"
    },
    {
      "cell_type": "code",
      "source": [
        "SWIFT_data.groupby('domain')['label'].value_counts()"
      ],
      "metadata": {
        "id": "YC0BpcIbwzXF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YC0BpcIbwzXF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task diagnosis"
      ],
      "metadata": {
        "id": "ubocseqWwzXF"
      },
      "id": "ubocseqWwzXF"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "# Initializing model\n",
        "model_to_finetunning = deepcopy(model)\n",
        "\n",
        "# Creating validation tasks\n",
        "valid = MetaTask(SWIFT_data,\n",
        "                 num_task = 500,\n",
        "                 k_support=10,\n",
        "                 k_query=20,\n",
        "                 tokenizer = Info['tokenizer'],\n",
        "                 training=False)\n",
        "\n",
        "# Diagnostic dataset\n",
        "i = valid.task_names =='Neuropain.csv'\n",
        "\n",
        "# Task index\n",
        "idx = np.array(range(len(i)))[i].item()\n",
        "\n",
        "# Support (train) e query (test) data\n",
        "support = valid[idx][0]\n",
        "query   = valid[idx][1]\n",
        "name   = valid[idx][2]\n",
        "\n",
        "print(name)\n",
        "print(SWIFT_data[SWIFT_data['domain']== name]['label'].value_counts())\n",
        "print(\"k_suport_examples:\",len(support))\n",
        "print(\"k_query_examples:\",len(query))\n",
        "\n",
        "# Support data loader\n",
        "support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                batch_size=5)\n"
      ],
      "metadata": {
        "id": "pP96xFMYwzXF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pP96xFMYwzXF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Untrained model"
      ],
      "metadata": {
        "id": "I9txQb_FwzXF"
      },
      "id": "I9txQb_FwzXF"
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_finetunning.to(device)\n",
        "\n",
        "# Inner Optimizer\n",
        "inner_optimizer = Adam(model_to_finetunning.parameters(), lr=5e-5)\n",
        "\n",
        "\n",
        "# Predicting\n",
        "model_to_finetunning.eval()\n",
        "with torch.no_grad():\n",
        "    query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "    query_batch = iter(query_dataloader).next()\n",
        "    query_batch = tuple(t.to(device) for t in query_batch)\n",
        "    q_input_ids, q_attention_mask, q_token_type_ids, q_label_id = query_batch\n",
        "    \n",
        "    # Predictions\n",
        "    _, features, predictions = model_to_finetunning(q_input_ids, q_attention_mask, q_token_type_ids, labels = q_label_id)\n",
        "\n",
        "    predictions = predictions.detach().cpu().squeeze()\n",
        "    q_label_id = q_label_id.detach().cpu()\n",
        "\n",
        "    acc = fn.accuracy(predictions, q_label_id).item()\n",
        "    print(\"acc:\",acc)\n",
        "\n",
        "# TSNE Dimensionality reduction\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(features[0].to('cpu'))\n",
        "\n",
        "# Plot\n",
        "sns.scatterplot(x=X_embedded[:, 0],\n",
        "                y=X_embedded[:, 1],\n",
        "                hue=q_label_id)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hh6PlZ7NwzXF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Hh6PlZ7NwzXF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trained model"
      ],
      "metadata": {
        "id": "7WbLwkFGwzXF"
      },
      "id": "7WbLwkFGwzXF"
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_finetunning = deepcopy(model)\n",
        "inner_optimizer = Adam(model_to_finetunning.parameters(), lr=5e-5)\n",
        "model_to_finetunning.train()\n",
        "model_to_finetunning.to(device)\n",
        "\n",
        "# Inner loop training\n",
        "for i in range(0,Info['inner_update_step']):\n",
        "    all_loss = []\n",
        "\n",
        "    # Inner training batch (support set)\n",
        "    for inner_step, batch in enumerate(support_dataloader):\n",
        "        \n",
        "        batch = tuple(t.to(\"cuda\") for t in batch)\n",
        "        input_ids, attention_mask, token_type_ids, label_id = batch\n",
        "\n",
        "        # Feed Foward\n",
        "        loss, _, _ = model_to_finetunning(input_ids, attention_mask, token_type_ids=token_type_ids, labels = label_id)\n",
        "                      \n",
        "        loss.backward()\n",
        "        inner_optimizer.step()\n",
        "        inner_optimizer.zero_grad()\n",
        "        \n",
        "        all_loss.append(loss.item())\n",
        "    \n",
        "    if i % Info[\"inner_print\"] == 0:\n",
        "        print(\"Inner Loss: \", np.mean(all_loss))\n",
        "\n",
        "# Predicting\n",
        "model_to_finetunning.eval()\n",
        "with torch.no_grad():\n",
        "    query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "    query_batch = iter(query_dataloader).next()\n",
        "    query_batch = tuple(t.to(device) for t in query_batch)\n",
        "    q_input_ids, q_attention_mask, q_token_type_ids, q_label_id = query_batch\n",
        "    \n",
        "    # Predictions\n",
        "    _, features, predictions = model_to_finetunning(q_input_ids, q_attention_mask, q_token_type_ids, labels = q_label_id)\n",
        "\n",
        "    predictions = predictions.detach().cpu().squeeze()\n",
        "    q_label_id = q_label_id.detach().cpu()\n",
        "\n",
        "    acc = fn.accuracy(predictions, q_label_id).item()\n",
        "    print(\"acc:\",acc)\n",
        "\n",
        "\n",
        "\n",
        "model_to_finetunning.to(torch.device('cpu'))\n",
        "del  inner_optimizer, model_to_finetunning\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# TSNE Dimensionality reduction\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random').fit_transform(features[0].to('cpu'))\n",
        "\n",
        "# Plot\n",
        "sns.scatterplot(x=X_embedded[:, 0],\n",
        "                y=X_embedded[:, 1],\n",
        "                hue=q_label_id,\n",
        "                alpha=torch.sigmoid(features[1]).to('cpu').view(-1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZhaBzCtZwzXF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZhaBzCtZwzXF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain learning phase in SWIFT datasets"
      ],
      "metadata": {
        "id": "LabRu1wowzXF"
      },
      "id": "LabRu1wowzXF"
    },
    {
      "cell_type": "code",
      "source": [
        "SWIFT_data.groupby('domain')['label'].value_counts()"
      ],
      "metadata": {
        "id": "IOSaoHXzwzXF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "IOSaoHXzwzXF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diagnosis"
      ],
      "metadata": {
        "id": "g7IemRYLwzXF"
      },
      "id": "g7IemRYLwzXF"
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "# Info = {\n",
        "#     \"inner_print\": 1, \n",
        "#     \"bert_layers\": 3,\n",
        "#     \"device\": torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "#     # \"model\": model,\n",
        "#     \"tokenizer\": initializer_bert.tokenizer,\n",
        "#     \"max_seq_length\": 512,\n",
        "#     \"meta_epoch\":30, # Numero de epocas do outerloop\n",
        "#     \"k_spt\":20, # Numero de treino por classe (k_spt) caso binario\n",
        "#     \"k_qry\":20, # Numero de valicao por classe (k_spt) caso binario\n",
        "#     \"outer_batch_size\": 5, # Divide as tasks em batchs tasks\n",
        "#     \"inner_batch_size\": 5, # Divide as classificacao em batch de classificao\n",
        "#     \"outer_update_lr\" : 5e-5, # Learning rate do otimizador das tasks\n",
        "#     \"inner_update_lr\" : 5e-5, # Learning rate do otimizador das classificao\n",
        "#     \"inner_update_step\" : 3, # Numero de epocas dentro do inner loop\n",
        "#     \"inner_update_step_eval\": 3, # Numero de epocas dentro do inner loop de validacao\n",
        "#     \"num_task_train\" : 100, # Quantidade de tarefas de treino\n",
        "#     \"num_task_test\" : 5 # Quantidade de tarefas de test\n",
        "# }\n",
        "\n",
        "Info['tresh'] = 0.9\n",
        "Info[\"inner_update_step_eval\"] = 3\n",
        "Info[\"inner_print\"] = 1"
      ],
      "metadata": {
        "id": "lhzinGLHwzXF"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lhzinGLHwzXF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful information:\n",
        "\n",
        "+ Variables should be specified within `Info`\n",
        "\n",
        "+ Model should be in the `model` variable\n",
        "\n",
        "+ Training should be done with specification in the `device` variable\n",
        "\n",
        "It's possible to access dome of the data from the testing dataset after pressing \"Train\", such as:\n",
        "\n",
        "+ `logits`: logits from classifier stage, no activation\n",
        "\n",
        "+ `X_embedded`: 2-dimensional values from dimensionality reduction of the latent space\n",
        "\n",
        "+ `features`: Latent space values  (feature_map layer output)\n",
        "\n",
        "+ `labels`: True values\n",
        "\n",
        "+ `data_train`: Training data (Not the same order given to the model on training)\n",
        "\n",
        "+ `data_test`: Testing data\n",
        "\n",
        "+ `batch_size_test`: Batch size from the testing dataset, so that prediction time can be reduced"
      ],
      "metadata": {
        "id": "zHxarGVmwzXF"
      },
      "id": "zHxarGVmwzXF"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluating validation dataset\n",
        "batch_size_test =   20 #@param {type:\"number\"}\n",
        "# Task names\n",
        "names = SWIFT_data['domain'].unique()\n",
        "\n",
        "diagnosis_comp = diagnosis(names, SWIFT_data, batch_size_test, model,Info, start=0)\n",
        "diagnosis_comp()"
      ],
      "metadata": {
        "id": "ztzWSxBFwzXG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ztzWSxBFwzXG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Avaliation of Positives examples"
      ],
      "metadata": {
        "id": "ENEiWehUwzXG"
      },
      "id": "ENEiWehUwzXG"
    },
    {
      "cell_type": "code",
      "source": [
        "def treat_text(text):\n",
        "    text = unicodedata.normalize(\"NFKD\",str(text))\n",
        "    text = multiple_replace(patterns,text.lower())\n",
        "    text = re.sub('(\\(.+\\))|(\\[.+\\])|( \\d )|(<)|(>)|(- )','', text)\n",
        "    text = re.sub('( +)',' ', text)\n",
        "    text = re.sub('(, ,)|(,,)',',', text)\n",
        "    text = re.sub('(%)|(per cent)',' percent', text)\n",
        "    return text\n",
        "\n",
        "data_test = diagnosis_comp.data_test\n",
        "logits = diagnosis_comp.logits\n",
        "\n",
        "# random choice a positive example\n",
        "indx= data_test[data_test['label'] == 'positive'].index\n",
        "indx= np.random.choice(indx)\n",
        "\n",
        "\n",
        "# print the example\n",
        "pprint(data_test.iloc[indx])\n",
        "\n",
        "# Prediction by the model\n",
        "print(\"Predicted:\",torch.sigmoid(logits[indx]).item() )\n",
        "\n",
        "# Text of the example\n",
        "print('Treated Text:')\n",
        "pprint(treat_text(data_test['text'].iloc[indx]))\n",
        "\n",
        "# Text of the example\n",
        "print('Text:')\n",
        "pprint(treat_text(data_test['text'].iloc[indx]))"
      ],
      "metadata": {
        "id": "XV7WbOpvwzXG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "XV7WbOpvwzXG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "dBIbP0JkwzXG"
      },
      "id": "dBIbP0JkwzXG"
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import datetime\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "base_path = 'Comparison'\n",
        "path_save = \"./\"+\"Models/\"+base_path+\"/\"\n",
        "\n",
        "\n",
        "\n",
        "# Creating directory\n",
        "Path(path_save).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Directory data\n",
        "check = (str(datetime.datetime.now()))[0:19]\n",
        "check = re.sub(\"[:-]\",\"_\",check)\n",
        "check = re.sub(\" \",\"_hr_\",check)\n",
        "\n",
        "print(\"Data e hora do salvamento:\", check)\n",
        "\n",
        "# Creating directory\n",
        "Path(f\"{path_save}/{check}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Saving paths\n",
        "model_path = f'{path_save}/{check}/model.pt'\n",
        "meta_info_path = f'{path_save}/{check}/Info.json'\n",
        "\n",
        "# Meta info\n",
        "save_info = Info.copy()\n",
        "save_info['model'] = initializer_model.tokenizer.name_or_path\n",
        "save_info.pop(\"tokenizer\")\n",
        "\n",
        "## Saving meta info\n",
        "with open(meta_info_path, 'w') as fp:\n",
        "    json.dump(save_info, fp)\n",
        "\n",
        "## Saving entire model\n",
        "torch.save(model, model_path)\n",
        "\n",
        "# To save only parameters:\n",
        "# torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "_eyoHAzqwzXG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_eyoHAzqwzXG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "mNRCgVUZwzXG"
      },
      "id": "mNRCgVUZwzXG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading entire model\n",
        "model = torch.load(model_path)\n",
        "model"
      ],
      "metadata": {
        "id": "mC4eab1ywzXG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mC4eab1ywzXG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing simulation attemps on SWIFT datasets"
      ],
      "metadata": {
        "id": "8wpA9YtGwzXG"
      },
      "id": "8wpA9YtGwzXG"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.clear_autocast_cache()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "path_save = \"./\"+\"Results/\"+base_path+\"/SWIFT/\"\n",
        "\n",
        "pipeline_simpulation(SWIFT_data, SWIFT_names, path_save, model, Info)"
      ],
      "metadata": {
        "id": "aagMZGmOwzXG"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aagMZGmOwzXG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing simulation attemps on cohen datasets"
      ],
      "metadata": {
        "id": "c00JK-eI_lOS"
      },
      "id": "c00JK-eI_lOS"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.clear_autocast_cache()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "path_save = \"./\"+\"Results/\"+base_path+\"/Drugs/\"\n",
        "\n",
        "pipeline_simpulation(cohen_data, cohen_names, path_save, model, Info)"
      ],
      "metadata": {
        "id": "9oppuIkh_lOd"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9oppuIkh_lOd"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deep-Learning",
      "language": "python",
      "name": "deep-learning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "Meta_learning_EFL.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9kxoLXspGo2C",
        "uZMuTuO6t1qq",
        "yV9633efG0Ql",
        "op1ERM4lqODR",
        "dd636d02-621e-4c60-845a-ddc41aef74b5",
        "jJwL3bIiIpIp",
        "9SYTNYLQqWPG",
        "4S0AvO8AqjV3"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}